{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar 1 \n",
    "\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T20:59:04.618627Z",
     "start_time": "2019-07-24T20:59:04.602978Z"
    }
   },
   "source": [
    "## üìö Objetivos de la clase üìö\n",
    "\n",
    "El objetivo principal de esta clase es introducirlos a la clasificaci√≥n de texto en NLP. \n",
    "Para esto, implementaremos varios modelos de clasificaci√≥n destinados a **predecir la categor√≠a de noticias de la radio biobio**.\n",
    "\n",
    "Los modelos y m√©todos que usaremos ser√°n las vistas en las clases anteriores: \n",
    "\n",
    "- Tokenizaci√≥n, Stemming, Lematizaci√≥n y eliminaci√≥n de Stop Words.\n",
    "- Bag of Words.\n",
    "- Claisifcador de Bayes .\n",
    "- Logistic regression.\n",
    "\n",
    "La clase estar√° enfocada en utilizar las siguientes librer√≠as (muy utilizadas en NLP):\n",
    "\n",
    "- Pandas\n",
    "- Scikit-Learn\n",
    "- Spacy\n",
    "- NLTK\n",
    "\n",
    "Una vez resuelto, pueden utilizar cualquier parte del c√≥digo que les parezca prudente para la tarea 1 (que tambi√©n es de clasificaci√≥n de texto! üòä).\n",
    "\n",
    "\n",
    "El notebook del auxiliar ya ejecutado se encuentra tanto en el [github](https://github.com/dccuchile/CC6205/tree/master/tutorials) del curso (Recuerden dejar su Star ‚≠êüòâ!), como en un colab de google. \n",
    "\n",
    "Para correr localmente el auxiliar, se recomienda instalar todos los paquetes usando anaconda,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar las librer√≠as\n",
    "\n",
    "### En local: Python y Conda\n",
    "\n",
    "Primero, si es que no tienen aun las librer√≠as, hay que instalarlas.\n",
    "Recuerden que usaremos `python 3.7` junto a `conda` como gestor de paquetes para el curso.\n",
    "\n",
    "Este pueden descargarlo e instalenlo desde aqu√≠ : [üêç Anaconda üêç](https://www.anaconda.com/distribution/).\n",
    "\n",
    "Para instalar las librer√≠as, ejecutar en una consola üíª:\n",
    "\n",
    "```cmd\n",
    "conda install pandas scikit-learn ntkl spacy \n",
    "```\n",
    "Y luego descargar el modelo de spacy en espa√±ol: \n",
    "\n",
    "```cmd\n",
    "python -m spacy download es_core_news_sm\n",
    "```\n",
    "\n",
    "Si saben un poco mas de anaconda, pueden instalar sus paquetes en un ambiente exlcusivo para el curso. Pero no es necesario!! Mas informaci√≥n [aqu√≠](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "### En colab\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "### Importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T22:34:18.517709Z",
     "start_time": "2020-04-22T22:34:18.497897Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T22:34:19.097975Z",
     "start_time": "2020-04-22T22:34:19.077561Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T22:34:35.094563Z",
     "start_time": "2020-04-22T22:34:33.887805Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "nlp = spacy.load(\"es_core_news_sm\", disable=['ner', 'parser', 'tagger'])\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T22:34:52.757813Z",
     "start_time": "2020-04-22T22:34:52.747748Z"
    }
   },
   "source": [
    "## Clasificaci√≥n de Texto "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øCu√°l de estos emails es SPAM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T23:05:11.304836Z",
     "start_time": "2020-04-22T23:05:11.285260Z"
    }
   },
   "source": [
    "Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T23:05:28.615278Z",
     "start_time": "2020-04-22T23:05:28.585246Z"
    }
   },
   "source": [
    "Pero, tambi√©n pueden representar otras clases..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øC√≥mo habr√°n encontrado la pel√≠cula?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, qu√© es la clasificaci√≥n de texto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento del texto\n",
    "\n",
    "En esta secci√≥n, cargaremos y los procesaremos el dataset de textos con el fin de representar de buena manera los datos para las tareas que debemos realizar. En esta, utilizaremos las siguientes t√©cnicas: \n",
    "\n",
    "- Tokenizaci√≥n\n",
    "- Eliminaci√≥n de Stopwords\n",
    "- Stemming\n",
    "- Lematizaci√≥n\n",
    "- Transformaci√≥n de Tokens a Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar los datasets \n",
    "\n",
    "\n",
    "\n",
    "Los datos que usaremos son 5 conjuntos de noticias extraidos de la radio biobio.\n",
    "Cada categor√≠a contiene 200 documentos (noticias). \n",
    "\n",
    "Los cargaremos utilizando la librer√≠a pandas: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:24:56.371042Z",
     "start_time": "2019-09-30T14:24:45.899614Z"
    }
   },
   "outputs": [],
   "source": [
    "nacional = pd.read_json(\"https://github.com/dccuchile/CC6205/releases/download/Data/biobio_nacional.json\", encoding ='utf-8')\n",
    "internacional = pd.read_json(\"https://github.com/dccuchile/CC6205/releases/download/Data/biobio_internacional.json\", encoding ='utf-8')\n",
    "economia = pd.read_json(\"https://github.com/dccuchile/CC6205/releases/download/Data/biobio_economia.json\", encoding ='utf-8')\n",
    "sociedad = pd.read_json(\"https://github.com/dccuchile/CC6205/releases/download/Data/biobio_sociedad.json\", encoding ='utf-8')\n",
    "opinion = pd.read_json(\"https://github.com/dccuchile/CC6205/releases/download/Data/biobio_opinion.json\", encoding ='utf-8')\n",
    "\n",
    "datasets = [nacional, internacional, economia, sociedad, opinion]\n",
    "dataset = pd.concat(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualizar que es lo que cargamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:24:56.792391Z",
     "start_time": "2019-09-30T14:24:56.729904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Por ejemplo, examinemos las columnas del dataset de noticias de la categor√≠a sociedad \n",
    "# (Todos los datasets tienen el mismo formato)\n",
    "sociedad.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de noticia de categor√≠a sociedad: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:24:57.136060Z",
     "start_time": "2019-09-30T14:24:57.120442Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extraigamos una noticia de ejemplo desde el dataset\n",
    "sample = sociedad.iloc[19:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:24:57.510972Z",
     "start_time": "2019-09-30T14:24:57.495350Z"
    }
   },
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:24:57.901509Z",
     "start_time": "2019-09-30T14:24:57.885885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extraemos el contenido y la categor√≠a de la noticia seleccionada.\n",
    "sample_content = sample.content.values[0]\n",
    "sample_category = sample.category.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:24:58.245175Z",
     "start_time": "2019-09-30T14:24:58.229583Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Contenido:\\n\\n\", sample_content.strip(),\n",
    "      \"\\n\\nClase Correspondiente:\\n\\n\", sample_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar\n",
    "\n",
    "¬øQu√© era tokenizar?\n",
    "\n",
    "    In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning).\n",
    "    \n",
    "Referencia: [Tokenizaci√≥n en wikipedia](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy y el objeto nlp\n",
    "\n",
    "`nlp` es el objeto que nos permite usar e interactuar con la librer√≠a [`spacy`](https://spacy.io/).\n",
    "Esta librer√≠a incluye variadas herramientras, tales como tokenizar, lematizar, descartar stopwords, entre otras (para este auxiliar, solo utilizaremos las mencionadas). El objeto nlp lo instanciamos en la secci√≥n de imports.\n",
    "\n",
    "Para usarla, simplemente se le pasa el texto como par√°metro, como veremos en el siguiente ejemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:00.292794Z",
     "start_time": "2019-09-30T14:25:00.261987Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in nlp(\"hola gent√≠l ciudadano, ¬øqu√© tal?\"):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizemos la noticia de ejemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:01.427050Z",
     "start_time": "2019-09-30T14:25:01.411393Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_content = [word.text for word in nlp(sample_content)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaci√≥n: \n",
    "\n",
    "    tokenized_content = [word.text for word in nlp(sample_content)] \n",
    "\n",
    "Equivale a :\n",
    "\n",
    "    tokenized_content = []\n",
    "    for word in nlp(sample_content):\n",
    "        tokenized_content.append(word.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinemos como qued√≥ el texto tokenizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:03.234029Z",
     "start_time": "2019-09-30T14:25:03.218411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tokenized_content)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que es b√°sicamente, un arreglo (o vector) con las palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:04.944758Z",
     "start_time": "2019-09-30T14:25:04.929138Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_content[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords \n",
    "\n",
    "¬øQu√© eran las stopwords?\n",
    "\n",
    "    In computing, stop words are words which are filtered out before or after processing of natural language data (text).[1] Stop words are generally the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools avoid removing stop words to support phrase search. \n",
    "    \n",
    "Referencias: [Stopwords en Wikipedia](https://en.wikipedia.org/wiki/Stop_words)\n",
    "\n",
    "En este caso, utilizaremos las stopwords inlcuidas en la librer√≠a spaCy en espa√±ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:06.571912Z",
     "start_time": "2019-09-30T14:25:06.556372Z"
    }
   },
   "outputs": [],
   "source": [
    "len(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:07.149914Z",
     "start_time": "2019-09-30T14:25:07.119086Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(STOP_WORDS).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remover las stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:08.774511Z",
     "start_time": "2019-09-30T14:25:08.758897Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_content_no_stop_words = [\n",
    "    token for token in tokenized_content if token not in STOP_WORDS\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:09.258773Z",
     "start_time": "2019-09-30T14:25:09.243152Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(tokenized_content, tokenized_content_no_stop_words),\n",
    "             columns=['original', 'no stopwords'])[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "¬øQu√© era el stemming? \n",
    "\n",
    "    Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form‚Äîgenerally a written word form.\n",
    "    \n",
    "Referencia: [Stemming en Wikipedia](https://en.wikipedia.org/wiki/Stemming)\n",
    "  \n",
    "#### Ejemplos: \n",
    "\n",
    "\n",
    "| word | stem of the word  |\n",
    "|---|---|\n",
    "working | work\n",
    "worked | work\n",
    "works | work\n",
    "\n",
    "#### nltk\n",
    "\n",
    "En este caso, utilizaremos la segunda librer√≠a de herramientas de nlp: [`nltk`](https://www.nltk.org/). Esta provee una buena herramienta para hacer stemming en espa√±ol : `SnowballStemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:10.905850Z",
     "start_time": "2019-09-30T14:25:10.874752Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "stemmed_content = [stemmer.stem(word) for word in tokenized_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:11.377944Z",
     "start_time": "2019-09-30T14:25:11.362347Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(tokenized_content, stemmed_content),\n",
    "             columns=['original', 'stem'])[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematizaci√≥n\n",
    "\n",
    "¬øQu√© era lematizaci√≥n? \n",
    "\n",
    "    \n",
    "    Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.[\n",
    "    \n",
    "    \n",
    "Referencia: [Lematizaci√≥n en wikipedia](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "    \n",
    "#### Ejemplos\n",
    "\n",
    "| word | lemma  |\n",
    "|---|---|\n",
    "dije| decir \n",
    "guapas | guapo\n",
    "mesa | mesas\n",
    "\n",
    "\n",
    "#### Lematizar el texto\n",
    "\n",
    "Al igual que la tokenizaci√≥n, utilizaremos `scpaCy` (a trav√©s del objeto `nlp`) para lematizar el contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:12.944616Z",
     "start_time": "2019-09-30T14:25:12.929000Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lemmatized_content = [word.lemma_ for word in nlp(sample_content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:13.350772Z",
     "start_time": "2019-09-30T14:25:13.319543Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizar la lematizaci√≥n\n",
    "pd.DataFrame(zip(tokenized_content, lemmatized_content),\n",
    "             columns=['original', 'lemma'])[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparativa entre stemming y lematizar\n",
    "\n",
    "Discusi√≥n: \n",
    "\n",
    "    ¬øCu√°l es mejor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:15.429293Z",
     "start_time": "2019-09-30T14:25:15.413785Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(tokenized_content, stemmed_content, lemmatized_content),\n",
    "             columns=['original', 'stem', 'lemma'])[1:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)\n",
    "\n",
    "¬øQu√© es?\n",
    "\n",
    "\n",
    "    \n",
    "    The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words \n",
    "    \n",
    "Referencia: [BoW en wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "\n",
    "### Ejemplo\n",
    "\n",
    "- Doc1 : 'I love dogs'\n",
    "- Doc2: 'I hate dogs and knitting.\n",
    "- Doc3: 'Knitting is my hobby and my passion.\n",
    "\n",
    "![BOW](https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2017/05/bagofwords.004.jpeg)\n",
    "\n",
    "### scikit-learn y CountVectorizer\n",
    "\n",
    "Para transformar los tokens al modelo `bag of words (BoW)` usaremos la librer√≠a [`scikit-learn`](https://scikit-learn.org/stable/), espec√≠ficamente, la clase [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "Esta requiere que definamos tokenizadores. Es decir, funciones que tomen un documento y lo retornen como una lista de palabras. \n",
    "\n",
    "En los tokenizadores podemos definir si dejar o quitar stopwords, lematizar, hacer stemming, entre otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizadores para CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:17.616318Z",
     "start_time": "2019-09-30T14:25:17.585041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizers for CountVectorizer\n",
    "\n",
    "\n",
    "# Solo tokenizar el doc.\n",
    "def tokenizer(doc):\n",
    "    return [x.orth_ for x in nlp(doc)]\n",
    "\n",
    "\n",
    "# Tokenizar y remover las stopwords del doc\n",
    "def tokenizer_with_stopwords(doc):\n",
    "    return [x.orth_ for x in nlp(doc) if x.orth_ not in STOP_WORDS]\n",
    "\n",
    "\n",
    "# Tokenizar y lematizar.\n",
    "def tokenizer_with_lemmatization(doc):\n",
    "    return [x.lemma_ for x in nlp(doc)]\n",
    "\n",
    "\n",
    "# Tokenizar y hacer stemming.\n",
    "def tokenizer_with_stemming(doc):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return [stemmer.stem(word) for word in [x.orth_ for x in nlp(doc)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:18.147409Z",
     "start_time": "2019-09-30T14:25:18.084950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instanciamos CountVectorizer con el tokenizador seleccionado. \n",
    "# Definimos si queremos n-gramas en el √∫ltimo par√°metro.\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             tokenizer=tokenizer,\n",
    "                             ngram_range=(1, 1))\n",
    "\n",
    "# Extraemos 4 noticias desde opinion y se la entregamos al vectorizador para que las transforme a vectores BoW.\n",
    "bow = vectorizer.fit_transform(opinion.sample(4).content)\n",
    "\n",
    "# Examinamos el primero\n",
    "bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:19.476356Z",
     "start_time": "2019-09-30T14:25:19.460735Z"
    }
   },
   "outputs": [],
   "source": [
    "#vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:19.960620Z",
     "start_time": "2019-09-30T14:25:19.944998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizemos el BoW generado para este caso\n",
    "bow[0].toarray()[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificaci√≥n de Texto\n",
    "\n",
    "### ¬øEn qu√© consiste la clasificaci√≥n de texto?\n",
    "\n",
    "Seg√∫n [wikipedia](https://en.wikipedia.org/wiki/Document_classification): \n",
    "\n",
    "    \"The task is to assign a document to one or more classes or categories\"\n",
    "\n",
    "\n",
    "Algunos ejemplos:\n",
    "\n",
    "- Assigning subject categories, **topics**, or genres\n",
    "- Spam detection \n",
    "- Authorship identification\n",
    "- Age/gender identification\n",
    "- Language Identification\n",
    "- Sentiment analysis\n",
    "- ...\n",
    "\n",
    "### Definici√≥n formal\n",
    "\n",
    "Input:\n",
    "- A document    $d$\n",
    "- A fixed set of classes $C=\\{c_{1},    c_{2},...,    c_{J}\\}$\n",
    "\n",
    "Output:    \n",
    "\n",
    "- A predicted class $c \\in C$\n",
    "\n",
    "### Tipos de t√©cnicas de clasificaci√≥n: \n",
    "\n",
    "- Hand-coded Rules. (No se ver√°n en este aux).\n",
    "- Supervised Machine Learning: \n",
    "    - Na√Øve Bayes\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Muchos mas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesar los datasets\n",
    "\n",
    "Seleccionar solo las columnas relevantes y divider en conjuntos de entrenamiento y de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:22.476867Z",
     "start_time": "2019-09-30T14:25:22.461256Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_datasets(dataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.content, dataset.category, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:22.929866Z",
     "start_time": "2019-09-30T14:25:22.914239Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = process_datasets(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificaci√≥n de t√≥picos de noticias con Naive Bayes\n",
    "\n",
    "\n",
    "¬øQu√© es?\n",
    "\n",
    "    In machine learning, naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features. \n",
    "\n",
    "- Simple (‚Äúna√Øve‚Äù) classification method based on Bayes rule that relies on very simple representation of document: *Bag of words*:\n",
    "\n",
    "Given a problem instance to be classified, represented by a vector $x =(x_{1},\\dots ,x_{n})$ representing some n features (independent variables), it assigns to this instance probabilities:\n",
    "\n",
    "$$ p(C_k | x_1, \\dots, x_n) $$\n",
    "\n",
    "or each of $K$ possible outcomes or classes $ C_{k}$.\n",
    "\n",
    "\n",
    "Using Bayes' theorem, the conditional probability can be decomposed as \n",
    "\n",
    "$$ p(C_k | x ) = \\frac{p (C_k) p(x | C_k)}{p(x)}$$\n",
    "\n",
    "In plain english:\n",
    "\n",
    "$$posterior = \\frac{prior * likehood}{evidence} $$\n",
    "\n",
    "\n",
    "#### En este caso...\n",
    "\n",
    "$$ p(\\ nacional\\ |\\  [0,0,3,0,6,0,2,\\dots] ) = \\frac{p (\\ nacional\\ )\\ p([0,0,3,0,6,0,2,\\dots]\\ | \\ nacional)}{p([0,0,3,0,6,0,2,\\dots])}$$\n",
    "\n",
    "\n",
    "Referencia : [Naive Bayes Classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "\n",
    "Observaci√≥n: Dado el supuesto de independencia del clasificador, no se deben utilizar n-gramas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establecer el Pipeline\n",
    "\n",
    "\n",
    "Un [`pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) es la definici√≥n del proceso que llevar√° a cabo el programa para preprocesarlo y despues clasificarlo. \n",
    "Puede tener m√∫ltiples etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:24.640099Z",
     "start_time": "2019-09-30T14:25:24.624478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Qu√© tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "# Definimos el vectorizador para convertir el texto a BoW:\n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer = TOKENIZER, ngram_range=(1,1))  \n",
    "\n",
    "# Definimos el clasificador que usaremos.\n",
    "clf = MultinomialNB()   \n",
    "\n",
    "# Definimos el pipeline\n",
    "text_clf = Pipeline([('vect', vectorizer), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenar\n",
    "\n",
    "Entrenamos el nuevo clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:28.596844Z",
     "start_time": "2019-09-30T14:25:25.769383Z"
    }
   },
   "outputs": [],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluar\n",
    "\n",
    "Evaluamos el rendimiento del clasificador que acabamos de entrenar, a traves de:\n",
    "\n",
    "- [`Matriz de confusi√≥n`](https://es.wikipedia.org/wiki/Matriz_de_confusi%C3%B3n)\n",
    "- [`Indice F1`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) (Puntaje, mientras mas alto mejor)\n",
    "\n",
    "En la matriz de confusi√≥n: \n",
    "\n",
    "- `precision` is the number of correct results divided by the number of all returned results. \n",
    "- `recall` is the number of correct results divided by the number of results that should have been returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:30.299583Z",
     "start_time": "2019-09-30T14:25:28.987382Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "conf = confusion_matrix(y_test, predicted)\n",
    "score = f1_score(y_test, predicted, average='macro') \n",
    "class_rep = classification_report(y_test, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nF1:'+str(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:30.705754Z",
     "start_time": "2019-09-30T14:25:30.690155Z"
    }
   },
   "outputs": [],
   "source": [
    "text_clf.predict([\"En puerto montt se encontr√≥ un perrito, que aparentemente, habr√≠a consumido drogas de alto calibre. Producto de esto, padecera severa ca√±a durante varios dias.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:31.111883Z",
     "start_time": "2019-09-30T14:25:31.096265Z"
    }
   },
   "outputs": [],
   "source": [
    "text_clf.predict([\"kim jong un ser√° el pr√≥ximo candidato a ministro de educaci√≥n.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:31.518075Z",
     "start_time": "2019-09-30T14:25:31.502464Z"
    }
   },
   "outputs": [],
   "source": [
    "text_clf.predict([\"El banco mundial present√≥ para chile un decrecimiento econ√≥mico de 92% y una inflaci√≥n de 8239832983289%.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T22:10:39.567162Z",
     "start_time": "2019-07-24T22:10:39.551542Z"
    }
   },
   "source": [
    "### Clasificaci√≥n de t√≥picos de noticias con Regresi√≥n Log√≠sitica\n",
    "\n",
    "No profundizaremos en este clasificador, mas del hecho de que se \"supone\" que deber√≠a tener mejor rendimiento que el de bayes.\n",
    "\n",
    "Referencia: [Regresi√≥n Log√≠stica](https://en.wikipedia.org/wiki/Logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establecer el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:32.811834Z",
     "start_time": "2019-09-30T14:25:32.796214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Qu√© tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "log_mod = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \n",
    "log_pipe = Pipeline([('vect', vectorizer), ('clf', log_mod)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:37.873152Z",
     "start_time": "2019-09-30T14:25:33.405472Z"
    }
   },
   "outputs": [],
   "source": [
    "log_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:39.513393Z",
     "start_time": "2019-09-30T14:25:38.248115Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted = log_pipe.predict(X_test)\n",
    "\n",
    "conf = confusion_matrix(y_test, predicted)\n",
    "score = f1_score(y_test, predicted, average='macro') \n",
    "class_rep = classification_report(y_test, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nF1 Score: '+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:39.903930Z",
     "start_time": "2019-09-30T14:25:39.888305Z"
    }
   },
   "outputs": [],
   "source": [
    "log_pipe.predict([\"En puerto montt se encontr√≥ un perrito, que aparentemente, habr√≠a consumido drogas de alto calibre. Producto de esto, padecera severa ca√±a durante varios dias.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:40.310082Z",
     "start_time": "2019-09-30T14:25:40.294466Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_pipe.predict([\"kim jong un ser√° el pr√≥ximo candidato a ministro de educaci√≥n.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificaci√≥n de Autor√≠a de documentos\n",
    "\n",
    "¬øExistir√° un patr√≥n en como escriben los periodistas que nos permitan identificarlos a partir de sus textos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:40.747481Z",
     "start_time": "2019-09-30T14:25:40.731895Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat(datasets).author.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:41.138046Z",
     "start_time": "2019-09-30T14:25:41.122394Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_datasets_by_author(datasets):\n",
    "    dataset = pd.concat(datasets)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.content, dataset.author, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:43.684295Z",
     "start_time": "2019-09-30T14:25:43.668673Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = process_datasets_by_author(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definir el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:44.465360Z",
     "start_time": "2019-09-30T14:25:44.449774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Qu√© tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "log_mod_by_author = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \n",
    "log_pipe_by_author = Pipeline([('vect', vectorizer), ('clf', log_mod_by_author)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:58.573064Z",
     "start_time": "2019-09-30T14:25:45.904175Z"
    }
   },
   "outputs": [],
   "source": [
    "log_pipe_by_author.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:26:00.355937Z",
     "start_time": "2019-09-30T14:25:58.996882Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted = log_pipe_by_author.predict(X_test_2)\n",
    "\n",
    "conf = confusion_matrix(y_test_2, predicted)\n",
    "score = f1_score(y_test_2, predicted, average='macro') \n",
    "class_rep = classification_report(y_test_2, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nF1 Score:'+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:28:03.045562Z",
     "start_time": "2019-08-08T13:28:03.029690Z"
    }
   },
   "source": [
    "## Cr√©ditos\n",
    "\n",
    "Todas las noticias extraidas perteneces a [Biobio Chile](https://www.biobiochile.cl/), los cuales gentilmente licencian todo su material a trav√©s de la [licencia Creative Commons (CC-BY-NC)](https://creativecommons.org/licenses/by-nc/2.0/cl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:28:21.929923Z",
     "start_time": "2019-08-08T13:28:21.917993Z"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "Gitgub del curso: \n",
    "- https://github.com/dccuchile/CC6205\n",
    "\n",
    "Slides:\n",
    "- https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf\n",
    "\n",
    "\n",
    "An√°lisis de sentimientos como clasificaci√≥n de texto:\n",
    "- https://affectivetweets.cms.waikato.ac.nz/benchmark/\n",
    "\n",
    "Algunos Recursos √∫tiles\n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "- [Scikit-learn Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
    "- [Spacy Tutorial](https://www.datacamp.com/community/blog/spacy-cheatsheet)\n",
    "- [NLTK Cheat sheet](http://sapir.psych.wisc.edu/programming_for_psychologists/cheat_sheets/Text-Analysis-with-NLTK-Cheatsheet.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "770px",
    "left": "1561px",
    "top": "111.133px",
    "width": "359px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
