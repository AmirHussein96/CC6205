{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar 1 \n",
    "\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T20:59:04.618627Z",
     "start_time": "2019-07-24T20:59:04.602978Z"
    }
   },
   "source": [
    "## üìö Objetivos de la clase üìö\n",
    "\n",
    "El objetivo principal de esta clase es introducirlos a la clasificaci√≥n de texto en NLP. \n",
    "Para esto, implementaremos varios modelos de clasificaci√≥n destinados a **predecir la categor√≠a de noticias de la radio biobio**.\n",
    "\n",
    "Los modelos y m√©todos que usaremos ser√°n las vistas en las clases anteriores: \n",
    "\n",
    "- Preprocesamiento: Tokenizaci√≥n, Stemming, Lematizaci√≥n y eliminaci√≥n de Stop Words.\n",
    "- Bag of Words.\n",
    "- Claisifcador de Bayes .\n",
    "- Logistic regression.\n",
    "\n",
    "La clase estar√° enfocada en utilizar las siguientes librer√≠as (muy utilizadas en NLP):\n",
    "\n",
    "- Pandas\n",
    "- Scikit-Learn\n",
    "- Spacy\n",
    "- NLTK\n",
    "\n",
    "Una vez resuelto, pueden utilizar cualquier parte del c√≥digo que les parezca prudente para la tarea 1 (que tambi√©n es de clasificaci√≥n de texto! üòä).\n",
    "\n",
    "\n",
    "\n",
    "El notebook del auxiliar ya ejecutado se encuentra tanto en el [github](https://github.com/dccuchile/CC6205/tree/master/tutorials) del curso (Recuerden dejar su Star ‚≠êüòâ!), como en un colab de google. \n",
    "\n",
    "\n",
    "TODO: Agregar referencias al video y a las diapos aqu√≠:\n",
    "\n",
    "Para correr localmente el auxiliar, se recomienda instalar todos los paquetes usando anaconda,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar las librer√≠as\n",
    "\n",
    "### En local: Python y Conda\n",
    "\n",
    "Primero, si es que no tienen aun las librer√≠as, hay que instalarlas.\n",
    "Recuerden que usaremos `python 3.7` junto a `conda` como gestor de paquetes para el curso.\n",
    "\n",
    "Este pueden descargarlo e instalenlo desde aqu√≠ : [üêç Anaconda üêç](https://www.anaconda.com/distribution/).\n",
    "\n",
    "Para instalar las librer√≠as, ejecutar en una consola üíª:\n",
    "\n",
    "```cmd\n",
    "conda install pandas scikit-learn ntkl spacy \n",
    "```\n",
    "Y luego descargar el modelo de spacy en espa√±ol: \n",
    "\n",
    "```cmd\n",
    "python -m spacy download es_core_news_sm\n",
    "```\n",
    "\n",
    "Si saben un poco mas de anaconda, pueden instalar sus paquetes en un ambiente exlcusivo para el curso. Pero no es necesario!! Mas informaci√≥n [aqu√≠](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "### En colab\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "### Importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T22:34:18.517709Z",
     "start_time": "2020-04-22T22:34:18.497897Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T22:34:19.097975Z",
     "start_time": "2020-04-22T22:34:19.077561Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T22:34:35.094563Z",
     "start_time": "2020-04-22T22:34:33.887805Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "nlp = spacy.load(\"es_core_news_sm\", disable=['ner', 'parser', 'tagger'])\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T22:34:52.757813Z",
     "start_time": "2020-04-22T22:34:52.747748Z"
    }
   },
   "source": [
    "## Clasificaci√≥n de Texto "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øCu√°l de estos emails es SPAM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![respuesta spam](https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/recursos/spam.PNG \"Email: Respuesta de cuales son spam y cuales no\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T23:05:11.304836Z",
     "start_time": "2020-04-22T23:05:11.285260Z"
    }
   },
   "source": [
    "La respuesta es..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![respuesta spam](https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/recursos/spam_2.PNG \"Email: Respuesta de cuales son spam y cuales no\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T23:05:28.615278Z",
     "start_time": "2020-04-22T23:05:28.585246Z"
    }
   },
   "source": [
    "Pero, estos tambi√©n pueden representar otros tipos de categor√≠as..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![categor√≠as de comunicados](https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/recursos/emails_clases.PNG \"Email: Cual es la clase del email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "¬øC√≥mo habr√°n encontrado la pel√≠cula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![¬øC√≥mo encontraron la pel√≠cula?](https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/recursos/limpiapiscinas.PNG \"Email: ¬øC√≥mo encontraron la pel√≠cula?\")\n",
    "\n",
    "\n",
    "Cr√©ditos a [OndaMedia](https://ondamedia.cl/). Pel√≠culas y mater√≠al audiovisual chileno gratis, muy recomendado!\n",
    "\n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, \n",
    "\n",
    "### ¬øQu√© es la clasificaci√≥n de texto?\n",
    "\n",
    "La clasificaci√≥n de texto consiste en tomar distintos textos y asignarles alguna clase. Dichas clases var√≠an seg√∫n la task que queramos resolver. Por ejemplo:\n",
    "\n",
    "    - Detectar emails SPAM -> SPAM, NO SPAM\n",
    "    - Reviews de Peliculas -> Buena, Mas o menos, Mala, Mal√≠sima, Brutalmente mala.\n",
    "    - An√°lisis de sentimientos de tweets: Felicidad, Tristeza, Enojo, Ira,...\n",
    "    - Detectar Fake News -> Es, No es\n",
    "    - Lenguaje del texto -> Espa√±ol, Ingl√©s, Chino,...\n",
    "    - Categor√≠a de una noticia -> Nacional, Internacional, Econom√≠a, Sociedad, Opini√≥n...\n",
    "\n",
    "Se define formalmente como:\n",
    "\n",
    "- Input: \n",
    "\n",
    "    - Un documento $d$\n",
    "    - Un conjunto fijo de clases $c_1, c_2, ..., c_j$\n",
    "\n",
    "- Output: \n",
    "    \n",
    "    - Una clase $c \\in C$ para el documento \n",
    "    \n",
    " \n",
    "Hay dos clases de m√©todos para resolver estos problemas: \n",
    "\n",
    "1. **Hand-coded Rules ü§ô**: \n",
    "\n",
    "Establecemos a mano las reglas que permiten detectar las clases.\n",
    "\n",
    "2. **Supervised Machine Learning üíª**:\n",
    "   \n",
    "Entrenamos clasificadores a partir de muchos ejemplos de documentos etiquetados a mano. \n",
    "\n",
    "----------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¬øQu√© haremos a continuaci√≥n?\n",
    "\n",
    "\n",
    "C√≥mo dijimos al comienzo, en este auxiliar crearemos un sistema que nos permita clasificar noticias de la radio biobio en 5 categor√≠as: `nacional`, `internacional`, `econom√≠a`, `sociedad`, `opinion`.\n",
    "\n",
    "Primero que nada, descargaremos los datos con los que trabajaremos.\n",
    "\n",
    "Luego, generaremos el sistema mas b√°sico, el cual consiste en transformar nuestro texto a `Bag of Words (BoW)` y luego, usar esos vectores para entrenar un clasificador. Este sistema nos puede entregar un muy buen baseline para comenzar a mejorar.\n",
    "\n",
    "A continuaci√≥n, veremos como mejorar aun mas nuestros resultados. Para esto agregaremos muchas mas t√©cnicas vistas en c√°tedra, tales como el `preprocesamiento de texto`, `la creaci√≥n de features personalizadas`, `reducci√≥n de dimensionalidad`, como tambi√©n probar con clasificadores a√∫n mas sofisticados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar los datasets \n",
    "\n",
    "\n",
    "Los datos que usaremos son 5 conjuntos de noticias obtenidas de la radio biobio.\n",
    "Cada categor√≠a contiene 200 documentos (noticias). \n",
    "\n",
    "Los cargaremos directamente desde el github del curso utilizando la librer√≠a `pandas` üêº: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:25:52.148271Z",
     "start_time": "2020-04-23T00:25:40.227535Z"
    }
   },
   "outputs": [],
   "source": [
    "nacional = pd.read_json(\"https://github.com/dccuchile/CC6205/releases/download/Data/biobio_nacional.json\", encoding ='utf-8')\n",
    "internacional = pd.read_json(\"https://github.com/dccuchile/CC6205/releases/download/Data/biobio_internacional.json\", encoding ='utf-8')\n",
    "economia = pd.read_json(\"https://github.com/dccuchile/CC6205/releases/download/Data/biobio_economia.json\", encoding ='utf-8')\n",
    "sociedad = pd.read_json(\"https://github.com/dccuchile/CC6205/releases/download/Data/biobio_sociedad.json\", encoding ='utf-8')\n",
    "opinion = pd.read_json(\"https://github.com/dccuchile/CC6205/releases/download/Data/biobio_opinion.json\", encoding ='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:29:02.897445Z",
     "start_time": "2020-04-23T00:29:02.867392Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([nacional, internacional, economia, sociedad, opinion])\n",
    "# Dejamos solo el titulo y el contenido y la clase\n",
    "dataset = dataset[['title', 'content', 'category']]\n",
    "# Ahora, mezclamos el titulo y el contenido, dejando as√≠ solo dos columnas:\n",
    "dataset['content'] = dataset['title'] + dataset['content']\n",
    "dataset = dataset.drop(columns=['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos unos cuantos ejemplos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:29:17.597253Z",
     "start_time": "2020-04-23T00:29:17.577417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Reino Unido descarta un intercambio de petrole...</td>\n",
       "      <td>Internacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Autoridad Palestina anuncia que dejar√° de resp...</td>\n",
       "      <td>Internacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Sename anuncia supervisi√≥n extraordinaria para...</td>\n",
       "      <td>Nacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Diputado Winter: No podemos seguir de brazos c...</td>\n",
       "      <td>Economia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Wifi en las casas de Cuba: la isla dio un paso...</td>\n",
       "      <td>Internacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Aseguran que director de Gendarmer√≠a sab√≠a \"ha...</td>\n",
       "      <td>Nacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Espacio P√∫blico y reuni√≥n de diputados con Pal...</td>\n",
       "      <td>Opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Quintero y Puchuncav√≠: estudio muestra que ni√±...</td>\n",
       "      <td>Nacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"Ah√≠ trabajan los retrasados\": burlas a trabaj...</td>\n",
       "      <td>Sociedad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>China es principal destino: exportaci√≥n de car...</td>\n",
       "      <td>Economia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content       category\n",
       "117  Reino Unido descarta un intercambio de petrole...  Internacional\n",
       "153  Autoridad Palestina anuncia que dejar√° de resp...  Internacional\n",
       "95   Sename anuncia supervisi√≥n extraordinaria para...       Nacional\n",
       "74   Diputado Winter: No podemos seguir de brazos c...       Economia\n",
       "94   Wifi en las casas de Cuba: la isla dio un paso...  Internacional\n",
       "36   Aseguran que director de Gendarmer√≠a sab√≠a \"ha...       Nacional\n",
       "156  Espacio P√∫blico y reuni√≥n de diputados con Pal...        Opinion\n",
       "29   Quintero y Puchuncav√≠: estudio muestra que ni√±...       Nacional\n",
       "22   \"Ah√≠ trabajan los retrasados\": burlas a trabaj...       Sociedad\n",
       "97   China es principal destino: exportaci√≥n de car...       Economia"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, el procedimiento estandar de mineria de datos de kinder: **Dividir nuestros conjuntos en train y test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:41:02.348263Z",
     "start_time": "2020-04-23T00:41:02.342278Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset.content,\n",
    "                                                    dataset.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:32:10.797379Z",
     "start_time": "2020-04-23T00:32:10.777275Z"
    }
   },
   "source": [
    "### Nuestro primer sistema de clasificaci√≥n\n",
    "\n",
    "\n",
    "Ahora que tenemos cargado el dataset, podemos implementar nuestro clasificador!\n",
    "\n",
    "Para esto, usaremos 3 herramientas fundamentales de scikit-learn: un `pipeline`, `CountVectorizer` y `MultinomialNB`.\n",
    "\n",
    "#### Pipeline\n",
    "\n",
    "\n",
    "Un [`pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) es la definici√≥n de los procesos que llevar√° a cabo el sistema que creemos. Nos permite tener unificados todos los procesos a la vez que simplifica el c√≥digo de nuestro sistema.\n",
    "\n",
    "\n",
    "#### Bag of Words y CountVectorizer\n",
    "\n",
    "\n",
    "¬øQu√© era Bag of Words?\n",
    "\n",
    "Es un modelo en donde transformamos cada una de las oraciones de nuestro dataset en vectores. Cada vector contiene una columna por cada palabra / **token** del vocabulario. Al procesar el dataset, cada oraci√≥n es mapeada a un vector que cuenta las apariciones de cada una de sus tokens. \n",
    "\n",
    "Referencia: [BoW en wikipedia](https://es.wikipedia.org/wiki/Modelo_bolsa_de_palabras)\n",
    "\n",
    "**Un peque√±o ejemplo**\n",
    "\n",
    "Supongamos que nuestro tokenizador solo separa por espacios.\n",
    "\n",
    "    - Doc1 : 'I love dogs'\n",
    "    - Doc2: 'I hate dogs and knitting.\n",
    "    - Doc3: 'Knitting is my hobby and my passion.\n",
    "\n",
    "El bag of words quedar√≠a:\n",
    "\n",
    "<img src=\"https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2017/05/bagofwords.004.jpeg\" alt=\"BoW\" style=\"width: 600px;\"/>\n",
    "\n",
    "`CountVectorizer` es la clase de `scikit` que transformar√° nuestro texto a Bag of Words. Fijense que es tremendamente √∫til tenerla dentro de un pipeline ya que fija en un comienzo el vocabulario que tendr√° el Bag of Words, evitando discordancias entre los vectores del conjunto de entrenamiento y el de prueba.\n",
    "\n",
    "#### MultinomialNB\n",
    "\n",
    "Simplemente, el cl√°sificador de Bayes.\n",
    "\n",
    "TODO: Explicar un poquito mas esto...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:44:48.540062Z",
     "start_time": "2020-04-23T00:44:48.536046Z"
    }
   },
   "source": [
    "------------------------\n",
    "\n",
    "\n",
    "**Primero, definimos el pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:41:37.436147Z",
     "start_time": "2020-04-23T00:41:37.426155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definimos el vectorizador para convertir el texto a BoW:\n",
    "vectorizer = CountVectorizer()  \n",
    "\n",
    "# Definimos el clasificador que usaremos.\n",
    "clf = MultinomialNB()   \n",
    "\n",
    "# Creamos el pipeline\n",
    "text_clf = Pipeline([('vect', vectorizer), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Luego, lo entrenamos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:41:50.876097Z",
     "start_time": "2020-04-23T00:41:50.586469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos nuestro pipeline\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Y predecimos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:42:15.436714Z",
     "start_time": "2020-04-23T00:42:15.301321Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:44:29.825754Z",
     "start_time": "2020-04-23T00:44:29.805883Z"
    }
   },
   "source": [
    "**Veamos como nos fue:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:46:51.985712Z",
     "start_time": "2020-04-23T00:46:51.965925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>predicted category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ranking de las 34 mejores empresas para j√≥vene...</td>\n",
       "      <td>Economia</td>\n",
       "      <td>Economia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Caso L√≠nea Azul: parlamentarios plantean imple...</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>Nacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Servicios Sanitarios niega lobby ante crisis d...</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>Nacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Rev√©s para Boris Johnson en su primera prueba ...</td>\n",
       "      <td>Internacional</td>\n",
       "      <td>Internacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Adolescente logra atrapar en el aire a ni√±a de...</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td>Sociedad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Accidente en Neuqu√©n: familiares de chilena fa...</td>\n",
       "      <td>Internacional</td>\n",
       "      <td>Nacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Corea del Norte dispara dos misiles de corto a...</td>\n",
       "      <td>Internacional</td>\n",
       "      <td>Internacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>A mitad de agosto se realizar√° nuevamente \"Rut...</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>Opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>A 20 aumentan muertos por tiroteo en centro co...</td>\n",
       "      <td>Internacional</td>\n",
       "      <td>Internacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Nuevo primer ministro Boris Johnson elige gabi...</td>\n",
       "      <td>Internacional</td>\n",
       "      <td>Internacional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content       category  \\\n",
       "25   Ranking de las 34 mejores empresas para j√≥vene...       Economia   \n",
       "72   Caso L√≠nea Azul: parlamentarios plantean imple...       Nacional   \n",
       "59   Servicios Sanitarios niega lobby ante crisis d...       Nacional   \n",
       "54   Rev√©s para Boris Johnson en su primera prueba ...  Internacional   \n",
       "167  Adolescente logra atrapar en el aire a ni√±a de...       Sociedad   \n",
       "181  Accidente en Neuqu√©n: familiares de chilena fa...  Internacional   \n",
       "162  Corea del Norte dispara dos misiles de corto a...  Internacional   \n",
       "137  A mitad de agosto se realizar√° nuevamente \"Rut...       Nacional   \n",
       "39   A 20 aumentan muertos por tiroteo en centro co...  Internacional   \n",
       "161  Nuevo primer ministro Boris Johnson elige gabi...  Internacional   \n",
       "\n",
       "    predicted category  \n",
       "25            Economia  \n",
       "72            Nacional  \n",
       "59            Nacional  \n",
       "54       Internacional  \n",
       "167           Sociedad  \n",
       "181           Nacional  \n",
       "162      Internacional  \n",
       "137            Opinion  \n",
       "39       Internacional  \n",
       "161      Internacional  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# algunos ejemplos:\n",
    "pd.DataFrame({'content': X_test, 'category':y_test, 'predicted category': y_pred}).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:46:52.955893Z",
     "start_time": "2020-04-23T00:46:52.930632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63,  0,  3,  4,  0],\n",
       "       [ 2, 68,  2,  2,  3],\n",
       "       [ 2,  0, 40, 15,  1],\n",
       "       [ 4,  1,  0, 61,  0],\n",
       "       [ 1,  7,  0,  2, 49]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usando la matriz de confusi√≥n:\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordatorio: \n",
    "TODO: Mejorar esto,.,,\n",
    "- `precision` is the number of correct results divided by the number of all returned results. \n",
    "- `recall` is the number of correct results divided by the number of results that should have been returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:46:53.685657Z",
     "start_time": "2020-04-23T00:46:53.665965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Economia       0.88      0.90      0.89        70\n",
      "Internacional       0.89      0.88      0.89        77\n",
      "     Nacional       0.89      0.69      0.78        58\n",
      "      Opinion       0.73      0.92      0.81        66\n",
      "     Sociedad       0.92      0.83      0.88        59\n",
      "\n",
      "     accuracy                           0.85       330\n",
      "    macro avg       0.86      0.85      0.85       330\n",
      " weighted avg       0.86      0.85      0.85       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usando el classification report:\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "Se ven bastante buenos los resultados. ¬øPero, podremos mejorarlos?\n",
    "\n",
    "### Preprocesamiento del texto\n",
    "\n",
    "En clases vimos que hab√≠an varias t√©cnicas que permiter preprocesar los textos.\n",
    "Es decir, c√≥mo hacemos el proceso de tokenizaci√≥n (separaci√≥n de las palabras).\n",
    "\n",
    "\n",
    "\n",
    "Alguna de las t√©cnicas son:\n",
    "\n",
    "\n",
    "- Eliminaci√≥n de Stopwords\n",
    "- Stemming\n",
    "- Lematizaci√≥n\n",
    "\n",
    "Existen otros preprocesadores que agregan informaci√≥n a las oraciones, tales como aquellos que indican negaciones.\n",
    "\n",
    "A continuaci√≥n, describiremos con mas detalle cada uno de estas t√©cnicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizar üî™\n",
    "\n",
    "¬øQu√© era tokenizar?\n",
    "\n",
    "\n",
    "    Es el proceso de convertir una secuencia de car√°cteres (por ejemplo, una oraci√≥n) en una secuencia de valores distintos entre si llamados tokens.\n",
    "    \n",
    "Referencia: [Tokenizaci√≥n en wikipedia](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### spaCy y el objeto nlp\n",
    "\n",
    "`nlp` es el objeto que nos permite usar e interactuar con la librer√≠a [`spacy`](https://spacy.io/).\n",
    "Esta librer√≠a incluye variadas herramientras, tales como tokenizar, lematizar, descartar stopwords, entre otras (para este auxiliar, solo utilizaremos las mencionadas). El objeto nlp lo instanciamos en la secci√≥n de imports.\n",
    "\n",
    "Para usarla, simplemente se le pasa el texto como par√°metro, como veremos en el siguiente ejemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T01:45:38.460422Z",
     "start_time": "2020-04-23T01:45:38.450612Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[quiero, salir, de, mi, casa, !, !, esto, no, es, un, meme, ., auxilio, ...]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOC = \"quiero salir de mi casa!! esto no es un meme. auxilio...\"\n",
    "\n",
    "tokens = []\n",
    "for word in nlp(DOC):\n",
    "    tokens.append(word)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T01:17:39.213266Z",
     "start_time": "2020-04-23T01:17:39.192949Z"
    }
   },
   "source": [
    "**Observaci√≥n**: Para este auxiliar usaremos `List Comprehension`, otra forma de hacer un for un poco mas reducida.\n",
    "Una muy buena referencia de esto [aqu√≠](https://www.programiz.com/python-programming/list-comprehension).\n",
    "\n",
    "La operaci√≥n anterior usando esta sint√°xis quedar√≠a como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T01:45:39.742086Z",
     "start_time": "2020-04-23T01:45:39.736115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[quiero, salir, de, mi, casa, !, !, esto, no, es, un, meme, ., auxilio, ...]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [word for word in nlp(DOC)]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords üõë\n",
    "\n",
    "¬øQu√© eran las stopwords?\n",
    "\n",
    "    Las Stopwords son palabras muy comunes en nuestro lenguaje y que por lo tanto, no aportan mucha informaci√≥n. Existen m√∫ltiples listas de stopwords para muchos idiomas y la aplicaci√≥n de estas variar√° caso a caso.\n",
    "\n",
    "    \n",
    "Referencias: [Stopwords en Wikipedia](https://en.wikipedia.org/wiki/Stop_words)\n",
    "\n",
    "En este caso, utilizaremos las stopwords inlcuidas en la librer√≠a spaCy en espa√±ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T01:25:11.166421Z",
     "start_time": "2020-04-23T01:25:11.162457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551\n",
      "['esas', 'siete', 'unas', 'diferente', 'muy', 'total', 'varias', 'dan', 'all√≠', 'aunque', 'cierto', 'repente', 'primeros', 'sino', 'sois', 'claro', 'era', 's√≠', 'considera', 'dias']\n"
     ]
    }
   ],
   "source": [
    "print(len(STOP_WORDS))\n",
    "print(list(STOP_WORDS)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "¬øQu√© era el stemming? \n",
    "\n",
    "    Son un conjunto de m√©todos enfocados en reducir cada palabra a su raiz.\n",
    "\n",
    "Referencia: [Stemming en Wikipedia](https://en.wikipedia.org/wiki/Stemming)\n",
    "  \n",
    "#### Ejemplos: \n",
    "\n",
    "\n",
    "| word | stem of the word  |\n",
    "|---|---|\n",
    "working | work\n",
    "worked | work\n",
    "works | work\n",
    "\n",
    "#### nltk\n",
    "\n",
    "En este caso, utilizaremos la segunda librer√≠a de herramientas de nlp: [`nltk`](https://www.nltk.org/). Esta provee una buena herramienta para hacer stemming en espa√±ol : `SnowballStemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T01:48:19.385491Z",
     "start_time": "2020-04-23T01:48:19.380485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quier', 'sal', 'de', 'mi', 'cas', '!', '!', 'esto', 'no', 'es', 'un', 'mem', '.', 'auxili', '...']\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "stemmed_doc = [stemmer.stem(str(token)) for token in tokens]\n",
    "print(stemmed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematizaci√≥n üôÄ\n",
    "\n",
    "¬øQu√© era lematizaci√≥n? \n",
    "\n",
    "    Es el proceso de transformar cada token a su lema, el cual es la palabra base sin ning√∫n tipo de flexi√≥n (o alteraci√≥n)\n",
    "    \n",
    "    \n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "Referencia: [Lematizaci√≥n en wikipedia](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "    \n",
    "#### Ejemplos\n",
    "\n",
    "| word | lemma  |\n",
    "|---|---|\n",
    "dije| decir \n",
    "guapas | guapo\n",
    "mesa | mesas\n",
    "\n",
    "\n",
    " <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Flexi%C3%B3nGato-svg.svg/300px-Flexi%C3%B3nGato-svg.svg.png\" alt=\"Flexi√≥n de gato\" style=\"width: 200px;\"/>\n",
    " \n",
    "\n",
    "#### Lematizar el texto\n",
    "\n",
    "Al igual que la tokenizaci√≥n, utilizaremos `scpaCy` (a trav√©s del objeto `nlp`) para lematizar el contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T02:00:08.147711Z",
     "start_time": "2020-04-23T02:00:08.143721Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['querer', 'salir', 'de', 'mi', 'casar', '!', '!', 'este', 'no', 'ser', 'uno', 'meme', '.', 'auxiliar', '...']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_content = [word.lemma_ for word in nlp(DOC)]\n",
    "print(lemmatized_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Discusi√≥n:**\n",
    "\n",
    "    ¬øCu√°l es mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregar nuestros tokenizadores al sistema\n",
    "\n",
    "Para agregar los tokenizadores en el sistema, los envolvemos en funciones que procesan independientemente cada texto. Luego, `CountVectorizer` se encargar√° de procesar cada documento del dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizadores para CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T02:02:19.964040Z",
     "start_time": "2020-04-23T02:02:19.949201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizers para CountVectorizer\n",
    "\n",
    "# Solo tokenizar el doc usando spacy.\n",
    "def tokenizer(doc):\n",
    "    return [x.orth_ for x in nlp(doc)]\n",
    "\n",
    "\n",
    "# Tokenizar y remover las stopwords del doc\n",
    "def tokenizer_with_stopwords(doc):\n",
    "    return [x.orth_ for x in nlp(doc) if x.orth_ not in STOP_WORDS]\n",
    "\n",
    "\n",
    "# Tokenizar y lematizar.\n",
    "def tokenizer_with_lemmatization(doc):\n",
    "    return [x.lemma_ for x in nlp(doc)]\n",
    "\n",
    "# Tokenizar y hacer stemming.\n",
    "def tokenizer_with_stemming(doc):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return [stemmer.stem(word) for word in [x.orth_ for x in nlp(doc)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos nuestro nuevo pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T02:02:54.398706Z",
     "start_time": "2020-04-23T02:02:54.368825Z"
    }
   },
   "outputs": [],
   "source": [
    "# seleccionamos el tokenizador a usar:\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "# Definimos el vectorizador para convertir el texto a BoW:\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             tokenizer=TOKENIZER,\n",
    "                             ngram_range=(1, 1))\n",
    "\n",
    "# Definimos el clasificador que usaremos.\n",
    "clf = MultinomialNB()   \n",
    "\n",
    "# Creamos el pipeline\n",
    "text_clf_2 = Pipeline([('vect', vectorizer), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamos nuestro pipeline y predecimos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T02:02:58.778898Z",
     "start_time": "2020-04-23T02:02:55.229118Z"
    }
   },
   "outputs": [],
   "source": [
    "text_clf_2.fit(X_train, y_train)\n",
    "y_pred = text_clf_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T02:02:59.178849Z",
     "start_time": "2020-04-23T02:02:59.158772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62  0  3  5  0]\n",
      " [ 2 68  2  3  2]\n",
      " [ 1  0 43 13  1]\n",
      " [ 4  1  0 61  0]\n",
      " [ 1  6  0  4 48]] \n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Economia       0.89      0.89      0.89        70\n",
      "Internacional       0.91      0.88      0.89        77\n",
      "     Nacional       0.90      0.74      0.81        58\n",
      "      Opinion       0.71      0.92      0.80        66\n",
      "     Sociedad       0.94      0.81      0.87        59\n",
      "\n",
      "     accuracy                           0.85       330\n",
      "    macro avg       0.87      0.85      0.85       330\n",
      " weighted avg       0.87      0.85      0.86       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usando la matriz de confusi√≥n:\n",
    "print(confusion_matrix(y_test, y_pred),\n",
    "      '\\n\\n-------------------------------------------------------\\n')\n",
    "# usando el classification report:\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T02:03:31.219102Z",
     "start_time": "2020-04-23T02:03:31.198936Z"
    }
   },
   "source": [
    "# ------------------------------ WIP --------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificaci√≥n de t√≥picos de noticias con Naive Bayes\n",
    "\n",
    "\n",
    "¬øQu√© es?\n",
    "\n",
    "    In machine learning, naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features. \n",
    "\n",
    "- Simple (‚Äúna√Øve‚Äù) classification method based on Bayes rule that relies on very simple representation of document: *Bag of words*:\n",
    "\n",
    "Given a problem instance to be classified, represented by a vector $x =(x_{1},\\dots ,x_{n})$ representing some n features (independent variables), it assigns to this instance probabilities:\n",
    "\n",
    "$$ p(C_k | x_1, \\dots, x_n) $$\n",
    "\n",
    "or each of $K$ possible outcomes or classes $ C_{k}$.\n",
    "\n",
    "\n",
    "Using Bayes' theorem, the conditional probability can be decomposed as \n",
    "\n",
    "$$ p(C_k | x ) = \\frac{p (C_k) p(x | C_k)}{p(x)}$$\n",
    "\n",
    "In plain english:\n",
    "\n",
    "$$posterior = \\frac{prior * likehood}{evidence} $$\n",
    "\n",
    "\n",
    "#### En este caso...\n",
    "\n",
    "$$ p(\\ nacional\\ |\\  [0,0,3,0,6,0,2,\\dots] ) = \\frac{p (\\ nacional\\ )\\ p([0,0,3,0,6,0,2,\\dots]\\ | \\ nacional)}{p([0,0,3,0,6,0,2,\\dots])}$$\n",
    "\n",
    "\n",
    "Referencia : [Naive Bayes Classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "\n",
    "Observaci√≥n: Dado el supuesto de independencia del clasificador, no se deben utilizar n-gramas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:30.299583Z",
     "start_time": "2019-09-30T14:25:28.987382Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "conf = confusion_matrix(y_test, predicted)\n",
    "score = f1_score(y_test, predicted, average='macro') \n",
    "class_rep = classification_report(y_test, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nF1:'+str(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T02:05:07.518914Z",
     "start_time": "2020-04-23T02:05:07.503770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sociedad'], dtype='<U13')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"En puerto montt se encontr√≥ un perrito, que aparentemente, habr√≠a consumido drogas de alto calibre. Producto de esto, padecera severa ca√±a durante varios dias.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T02:05:05.639017Z",
     "start_time": "2020-04-23T02:05:05.628912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Internacional'], dtype='<U13')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"kim jong un ser√° el pr√≥ximo candidato a ministro de educaci√≥n.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T02:05:03.668908Z",
     "start_time": "2020-04-23T02:05:03.658890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Economia'], dtype='<U13')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"El banco mundial present√≥ para chile un decrecimiento econ√≥mico de 92% y una inflaci√≥n de 8239832983289%.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T22:10:39.567162Z",
     "start_time": "2019-07-24T22:10:39.551542Z"
    }
   },
   "source": [
    "### Clasificaci√≥n de t√≥picos de noticias con Regresi√≥n Log√≠sitica\n",
    "\n",
    "No profundizaremos en este clasificador, mas del hecho de que se \"supone\" que deber√≠a tener mejor rendimiento que el de bayes.\n",
    "\n",
    "Referencia: [Regresi√≥n Log√≠stica](https://en.wikipedia.org/wiki/Logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establecer el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:32.811834Z",
     "start_time": "2019-09-30T14:25:32.796214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Qu√© tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "log_mod = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \n",
    "log_pipe = Pipeline([('vect', vectorizer), ('clf', log_mod)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:37.873152Z",
     "start_time": "2019-09-30T14:25:33.405472Z"
    }
   },
   "outputs": [],
   "source": [
    "log_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:39.513393Z",
     "start_time": "2019-09-30T14:25:38.248115Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted = log_pipe.predict(X_test)\n",
    "\n",
    "conf = confusion_matrix(y_test, predicted)\n",
    "score = f1_score(y_test, predicted, average='macro') \n",
    "class_rep = classification_report(y_test, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nF1 Score: '+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:39.903930Z",
     "start_time": "2019-09-30T14:25:39.888305Z"
    }
   },
   "outputs": [],
   "source": [
    "log_pipe.predict([\"En puerto montt se encontr√≥ un perrito, que aparentemente, habr√≠a consumido drogas de alto calibre. Producto de esto, padecera severa ca√±a durante varios dias.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:40.310082Z",
     "start_time": "2019-09-30T14:25:40.294466Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_pipe.predict([\"kim jong un ser√° el pr√≥ximo candidato a ministro de educaci√≥n.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificaci√≥n de Autor√≠a de documentos\n",
    "\n",
    "¬øExistir√° un patr√≥n en como escriben los periodistas que nos permitan identificarlos a partir de sus textos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:40.747481Z",
     "start_time": "2019-09-30T14:25:40.731895Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat(datasets).author.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:41.138046Z",
     "start_time": "2019-09-30T14:25:41.122394Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_datasets_by_author(datasets):\n",
    "    dataset = pd.concat(datasets)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.content, dataset.author, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:43.684295Z",
     "start_time": "2019-09-30T14:25:43.668673Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = process_datasets_by_author(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definir el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:44.465360Z",
     "start_time": "2019-09-30T14:25:44.449774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Qu√© tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "log_mod_by_author = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \n",
    "log_pipe_by_author = Pipeline([('vect', vectorizer), ('clf', log_mod_by_author)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:25:58.573064Z",
     "start_time": "2019-09-30T14:25:45.904175Z"
    }
   },
   "outputs": [],
   "source": [
    "log_pipe_by_author.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:26:00.355937Z",
     "start_time": "2019-09-30T14:25:58.996882Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted = log_pipe_by_author.predict(X_test_2)\n",
    "\n",
    "conf = confusion_matrix(y_test_2, predicted)\n",
    "score = f1_score(y_test_2, predicted, average='macro') \n",
    "class_rep = classification_report(y_test_2, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nF1 Score:'+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:28:03.045562Z",
     "start_time": "2019-08-08T13:28:03.029690Z"
    }
   },
   "source": [
    "## Cr√©ditos\n",
    "\n",
    "Todas las noticias extraidas perteneces a [Biobio Chile](https://www.biobiochile.cl/), los cuales gentilmente licencian todo su material a trav√©s de la [licencia Creative Commons (CC-BY-NC)](https://creativecommons.org/licenses/by-nc/2.0/cl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:28:21.929923Z",
     "start_time": "2019-08-08T13:28:21.917993Z"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "Gitgub del curso: \n",
    "- https://github.com/dccuchile/CC6205\n",
    "\n",
    "Slides:\n",
    "- https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf\n",
    "\n",
    "\n",
    "An√°lisis de sentimientos como clasificaci√≥n de texto:\n",
    "- https://affectivetweets.cms.waikato.ac.nz/benchmark/\n",
    "\n",
    "Algunos Recursos √∫tiles\n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "- [Scikit-learn Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
    "- [Spacy Tutorial](https://www.datacamp.com/community/blog/spacy-cheatsheet)\n",
    "- [NLTK Cheat sheet](http://sapir.psych.wisc.edu/programming_for_psychologists/cheat_sheets/Text-Analysis-with-NLTK-Cheatsheet.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "770px",
    "left": "1561px",
    "top": "111.133px",
    "width": "359px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
