{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auxiliar 5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6912a31b002244ba9878dd0f5e879e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e3a3aa3148d14161a9bfaab79f6c2498",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5a92f2c65b5d44bdb55db4c70a10acbd",
              "IPY_MODEL_8d9fa6bb830b4c968ed07cad332a253f"
            ]
          }
        },
        "e3a3aa3148d14161a9bfaab79f6c2498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a92f2c65b5d44bdb55db4c70a10acbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_76c538b27def4a6d8f33d28924d27210",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15c358c34e7d4d179ad69203683b1f57"
          }
        },
        "8d9fa6bb830b4c968ed07cad332a253f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9cec363e9c1945aca4cd65895fff9d62",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 373kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_145b9b82731d4611a27a23614181fbf2"
          }
        },
        "76c538b27def4a6d8f33d28924d27210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15c358c34e7d4d179ad69203683b1f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9cec363e9c1945aca4cd65895fff9d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "145b9b82731d4611a27a23614181fbf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d8f1ea3f6f2547c8b4bf6db95c783022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4ccdda4320954b878078608871d4533b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_afb2f2d2e3f741c69c24c9e60cb4a3db",
              "IPY_MODEL_68b29443ce81485f99e3e889555d432c"
            ]
          }
        },
        "4ccdda4320954b878078608871d4533b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afb2f2d2e3f741c69c24c9e60cb4a3db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8e6613fe1852449c95fbb92810bfdcd0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83570edef12947c9a3f27b6d7c3d38fe"
          }
        },
        "68b29443ce81485f99e3e889555d432c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_225e29d32b70439ebf461b4219d3c12e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:10&lt;00:00, 41.8B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4f28403a735a48c3b91ede9cfd9d417d"
          }
        },
        "8e6613fe1852449c95fbb92810bfdcd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83570edef12947c9a3f27b6d7c3d38fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "225e29d32b70439ebf461b4219d3c12e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4f28403a735a48c3b91ede9cfd9d417d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af2f62aec57d4beea2f6294cc0dda376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ba4d288e623f4ca58f094100788abcd4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_88613a49b3e3438cb452eead274a52a1",
              "IPY_MODEL_dc8fbc48b3ee40f08f1ea6adb7d73ef7"
            ]
          }
        },
        "ba4d288e623f4ca58f094100788abcd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88613a49b3e3438cb452eead274a52a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1f3d10e74b0c4dd0a7cb060e3e9d43fb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b26447849a5d48b6bc3171b70c44f9b2"
          }
        },
        "dc8fbc48b3ee40f08f1ea6adb7d73ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5f545f04bcfd4843a103f92b85188151",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:09&lt;00:00, 44.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d283019c74bc405aadd9bfddc72e4008"
          }
        },
        "1f3d10e74b0c4dd0a7cb060e3e9d43fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b26447849a5d48b6bc3171b70c44f9b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f545f04bcfd4843a103f92b85188151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d283019c74bc405aadd9bfddc72e4008": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLLTxJRSwscP",
        "colab_type": "text"
      },
      "source": [
        "#IntroducciÃ³n\n",
        "\n",
        "<!-- AquÃ­ la idea es mostrarles como pueden usar BERT y quizÃ¡s BETO en\n",
        "espaÃ±ol usando la librerÃ­a Transformers. Me interesa que lo usen\n",
        "de dos formas:\n",
        "\n",
        "1) como extractor de vectores contextualizados\n",
        "\n",
        "2) para hacer fine-tuning a otra task (e.g., Question Answering).\n",
        " -->\n",
        "\n",
        "\n",
        "------------------------------------------------------\n",
        "En esta auxiliar vamos a utilizar BERT, un modelo de lenguaje desarrollado por Google. Este modelo rompiÃ³ varios rÃ©cords en NLP y de hecho, cada vez que buscan en Google, BERT ayuda a refinar sus bÃºsquedas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2EvTlRNbfSV",
        "colab_type": "text"
      },
      "source": [
        "## Pero, quÃ© es BERT?\n",
        "\n",
        "BERT es un personaje de plaza sÃ©samo, al igual que ELMo, los cuales saben mucho de lenguaje y podemos ver con una mirada desafiante en la siguiente imagen:\n",
        "\n",
        "![bert y elmo](https://i.imgur.com/1T4kyrq.png)\n",
        "\n",
        "Los genios dÃ¡ndole nombres a los papers decidieron que era buena idea que los acrÃ³nimos se refirieran a los personajes de plaza sesamo, con los cuales algunos de nosotros (los mÃ¡s viejos) aprendimos a hablar y deletrear ~~hasta quizÃ¡s mejor que en el jardÃ­n infantil~~. Al igual que Elmo, Embeddings from Language MOdels, BERT es el acrÃ³nimo de Bidirectional Encoder Representations from Transformers. \\\n",
        "Estos dos modelos producen \"contextualized word embeddings\". A diferencia de los modelos que producen static word embeddings como Word2Vec, la representaciÃ³n no depende solo de la palabra, sino que de la palabra y su contexto. Por lo tanto, cada palabra tiene infinitas representaciones, lo cual es mucho mÃ¡s flexible que tener solo un vector para cada palabra.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woZQw3j2bkPw",
        "colab_type": "text"
      },
      "source": [
        "## QuÃ© significa Bidirectional Encoder Representations from Transformers?\n",
        "A diferencia de ELMo, el cual era una concatenaciÃ³n de informaciÃ³n de izquierda-derecha y derecha-izquierda, BERT es bidireccional, es decir, toma en cuenta los contextos a la izquierda y derecha de la palabra simultÃ¡neamente.\n",
        "\n",
        "BERT ademÃ¡s utiliza Transformers, arquitecturas de deep learning altamente paralelizables que cuentan con un proceso de Encoder-Decoder. Dado que el objetivo de BERT es generar un modelo de lenguaje, solo es necesario el mecanismo de Encoding y le dejan el proceso de Decoding a las distintas tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-CrbVL5bsrm",
        "colab_type": "text"
      },
      "source": [
        "## Y como fue entrenado?\n",
        "\n",
        "El primer objetivo de BERT es algo que se llama \"masked language modeling\". En este modelo, las palabras de una frase se borran al azar y se reemplazan por un token especial ([MASK]) con probabilidad 15%. Luego, se utiliza un Transformer para generar una predicciÃ³n para la palabra remplazada por [MASK] basada en las palabras no enmascaradas que la rodean, tanto a la izquierda como a la derecha.\n",
        "\n",
        "El segundo objetivo de BERT es resolver la tarea de Next Sentence Prediction. El modelo recibe dos oraciones como entrada y aprende a predecir si la segunda oraciÃ³n del par es la oraciÃ³n que siguiente del documento original. Durante el entrenamiento, el 50% de los inputs son un par en el que la segunda frase es la frase siguiente en el documento original, mientras que en el otro 50% se elige una frase aleatoria del corpus como segunda frase.\n",
        "\n",
        "\n",
        "Pueden leer un poco mÃ¡s [acÃ¡](http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqCm5gsibx5s",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Oye, pero esto suena un poco magico, tienes algunos ejemplos?\n",
        "\n",
        "Hay bastantes librerÃ­as que tienen el modelo pre-entrenado a disposiciÃ³n, partiendo por el [GitHub de BERT](https://github.com/google-research/bert) implementado en TensorFlow. Como nosotros sabemos utilizar pytorch, utilizaremos la [version de HuggingFace](https://huggingface.co/transformers/) la cual es respaldada por el github de Google y la elogian: \"which is compatible with our pre-trained checkpoints and is able to reproduce our results\". Esta version se importa con la libreria transformers. Otras version disponibles son [sentence-bert](https://github.com/UKPLab/sentence-transformers) o [bert-as-service](https://github.com/hanxiao/bert-as-service)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXbAJvFZdRK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "outputId": "cbb46b63-54a4-4968-ddeb-4c501a0e1221"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 778kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 10.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 20.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 24.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=3e4be8913ce14034d23d163eeaa1cace8c4eb7f697259a6155d68843f94cd723\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsaS37KAzZoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcHpz3iEc6-_",
        "colab_type": "text"
      },
      "source": [
        "Veamos el primer ejemplo que entrega la documentaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HePHI6odsBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165,
          "referenced_widgets": [
            "6912a31b002244ba9878dd0f5e879e65",
            "e3a3aa3148d14161a9bfaab79f6c2498",
            "5a92f2c65b5d44bdb55db4c70a10acbd",
            "8d9fa6bb830b4c968ed07cad332a253f",
            "76c538b27def4a6d8f33d28924d27210",
            "15c358c34e7d4d179ad69203683b1f57",
            "9cec363e9c1945aca4cd65895fff9d62",
            "145b9b82731d4611a27a23614181fbf2",
            "d8f1ea3f6f2547c8b4bf6db95c783022",
            "4ccdda4320954b878078608871d4533b",
            "afb2f2d2e3f741c69c24c9e60cb4a3db",
            "68b29443ce81485f99e3e889555d432c",
            "8e6613fe1852449c95fbb92810bfdcd0",
            "83570edef12947c9a3f27b6d7c3d38fe",
            "225e29d32b70439ebf461b4219d3c12e",
            "4f28403a735a48c3b91ede9cfd9d417d",
            "af2f62aec57d4beea2f6294cc0dda376",
            "ba4d288e623f4ca58f094100788abcd4",
            "88613a49b3e3438cb452eead274a52a1",
            "dc8fbc48b3ee40f08f1ea6adb7d73ef7",
            "1f3d10e74b0c4dd0a7cb060e3e9d43fb",
            "b26447849a5d48b6bc3171b70c44f9b2",
            "5f545f04bcfd4843a103f92b85188151",
            "d283019c74bc405aadd9bfddc72e4008"
          ]
        },
        "outputId": "77141544-720a-4a78-f854-373740814336"
      },
      "source": [
        "# Si estamos utilizando google colab, no se preocupen por las descargas, ya que las hace el servidor de colab y no les gasta ancho de banda a uds\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Cargamos el tokenizador\n",
        "model = BertModel.from_pretrained('bert-base-uncased') # Cargamos el modelo pre-entrenado"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6912a31b002244ba9878dd0f5e879e65",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8f1ea3f6f2547c8b4bf6db95c783022",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af2f62aec57d4beea2f6294cc0dda376",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwxJlH5Vc914",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "e33bb411-399c-4de1-8494-b7e3ec29ab1c"
      },
      "source": [
        "# 'pt' especifica que queremos vectores de pytorch, 'tf' seria en tensorflow\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\") \n",
        "# el doble asterico ayuda a evaluar los valores de un diccionario, por ejemplo:\n",
        "# d = {'a': 1, 'b':2}, model(**d) serÃ­a equivalente a model(1,2)\n",
        "outputs = model(**inputs)\n",
        "# El ultimo hidden-state es el primer elemento del output del modelo.\n",
        "last_hidden_states = outputs[0].squeeze(0) # squeeze en la primera dimension ya que es 1\n",
        "print(inputs['input_ids']) # Tenemos 8 tokens, contando el CLS (101) y el SEP (102)\n",
        "print(last_hidden_states.shape) # Tenemos 8 vectores de 768 dimensiones\n",
        "print(last_hidden_states)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]])\n",
            "torch.Size([8, 768])\n",
            "tensor([[-0.1144,  0.1937,  0.1250,  ..., -0.3827,  0.2107,  0.5407],\n",
            "        [ 0.5308,  0.3207,  0.3665,  ..., -0.0036,  0.7579,  0.0388],\n",
            "        [-0.4877,  0.8849,  0.4256,  ..., -0.6976,  0.4458,  0.1231],\n",
            "        ...,\n",
            "        [-0.7003, -0.1815,  0.3297,  ..., -0.4838,  0.0680,  0.8901],\n",
            "        [-1.0355, -0.2567, -0.0317,  ...,  0.3197,  0.3999,  0.1795],\n",
            "        [ 0.6080,  0.2610, -0.3131,  ...,  0.0311, -0.6283, -0.1994]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU4iTHzpbBjm",
        "colab_type": "text"
      },
      "source": [
        "La primera pregunta es... cÃ³mo diantres obtengo una representaciÃ³n de mi oraciÃ³n desde el Ãºltimo hidden-state?\n",
        "\n",
        "Las opciones mÃ¡s simples son tomar el token CLS, aunque no es muy recomendado, ya que depende del fine tunning (veremos esto mÃ¡s adelante) y la otra opciÃ³n es tomar el promedio de todos mis tokens. Hay mÃ¡s formas de pooling (es decir como se mezclan los tokens), por ejemplo, podemos ver las de bert-as-service [acÃ¡](https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbmxaTNpeeCI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0d22a0b5-ad8c-4ba3-e295-5c407e33cc2e"
      },
      "source": [
        "cantidad_tokens = inputs['input_ids'].shape[1]\n",
        "# Representacion con token cls\n",
        "cls_representation = last_hidden_states[0] # El primer token del ultimo hidden-state es el CLS\n",
        "print(cls_representation.shape) # Representacion de 768 dimensiones\n",
        "\n",
        "# Representacion con average 1 (mÃ¡s verbosa)\n",
        "average = torch.zeros(768)\n",
        "for i in range(1, cantidad_tokens-1): # Partimos en 1 y terminamos en largo-2 para ignorar CLS y SEP\n",
        "  average += last_hidden_states[i] \n",
        "average = average/(cantidad_tokens-2) # Obtenemos nuestra representacion de 768 dimensiones\n",
        "print(average.shape)\n",
        "\n",
        "# Representacion con average 2 (mÃ¡s corta)\n",
        "average2 = torch.mean(last_hidden_states[1:-1], 0)\n",
        "print(average2.shape)\n",
        "print(torch.equal(average, average2))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([768])\n",
            "torch.Size([768])\n",
            "torch.Size([768])\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noj-NgUarIXt",
        "colab_type": "text"
      },
      "source": [
        "## BacÃ¡n, puedo representar oraciÃ³nes con BERT, puedo hacer algo mÃ¡s a parte de eso o se acabÃ³ la diversiÃ³n?\n",
        "\n",
        "Â¡Esto es solo el principio! Â¡La librerÃ­a transformers a parte nos presta modelos con decoders fine-tuneados en ciertas tareas y hasta podemos fine-tunearlos nosotros!\n",
        "\n",
        "Fine-tunear ayuda a que BERT entienda cual es la tarea que queremos resolver. El modelo de por sÃ­ ya viene pre-entrenado en las 2 tareas que mencione previamente, Masked Language Modeling y Next Sentence Prediction sobre corpus muy grandes. \n",
        "\n",
        "Hay 2 modelos pre-entrenados de BERT: bert-base y bert-large que difieren en el tamaÃ±o del modelo, pero fueron entrenados sobre el mismo corpus: Wikipedia en ingles, ademÃ¡s de aproximadamente 11.000 libros en ingles (esto se llama BookCorpus).\n",
        "\n",
        "bert-base tiene 12 layers (transformer blocks), 12 attention heads, y 110 millones de parametros\n",
        "\n",
        "bert-large tiene 24 layers (transformer blocks), 16 attention heads, y 340 millones de parametros\n",
        "\n",
        "Una vez hice el cÃ¡lculo rÃ¡pido de cuanto me saldrÃ­a entrenar bert-base desde cero en las cloud TPU de Google y era una cifra cercana a los 2000 dÃ³lares. Por suerte aquÃ­ tenemos a alguien que lo entrenÃ³ en espaÃ±ol y nos puede contar su experiencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3in0Nv1fZ9N",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Gabriel y BETO\n",
        "\n",
        "Experiencia de Gabriel y BETO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbgGNC8bsOp3",
        "colab_type": "text"
      },
      "source": [
        "Continuando con los ejemplos, veamos como serÃ­a utilizar BertForNextSentencePrediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5Hgj5mtszxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForNextSentencePrediction"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiREjwJXs_J_",
        "colab_type": "text"
      },
      "source": [
        "Utilizamos el tokenizador comÃºn de BERT, solo cambiamos el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fImmBe-es7MS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "5efefdeb-f12f-458e-fbd9-fc3233cba629"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA1PyIHxtEl3",
        "colab_type": "text"
      },
      "source": [
        "Creemos una funciÃ³n que nos dice si tiene sentido o no la oraciÃ³n que continua."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykgyF3hrtSes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluar_oraciones(primera,segunda):\n",
        "  encoding = tokenizer(primera, segunda, return_tensors='pt')\n",
        "  loss, logits = model(**encoding, next_sentence_label=torch.LongTensor([1])) # El label representa cual es la oraciÃ³n\n",
        "  #Nota logits[0,0] entrega el score que la oracion si sea la siguiente (que tan True)\n",
        "  #logits[0,1] entrega el score de que la oracion no sea la siguiente (que tan False)\n",
        "  # Se puede aplicar una SoftMax sobre estos resultados para que sean probabilidades\n",
        "  # Pero no es necesario.\n",
        "  if logits[0, 0] < logits[0, 1]:\n",
        "    print(\"La oraciÃ³n no tiene nada que ver\")\n",
        "  elif logits[0,0] > logits[0,1]:\n",
        "    print(\"La oraciÃ³n es una continuaciÃ³n\")\n",
        "  else:\n",
        "    print(\"No estoy seguro\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-RxdXlvuPW9",
        "colab_type": "text"
      },
      "source": [
        "Probemos este cÃ³digo con algunos ejemplos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNNxshA5tK8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c8f9ac0-0ae8-46b3-f370-17cf3e2e4b78"
      },
      "source": [
        "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
        "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
        "evaluar_oraciones(prompt,next_sentence)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La oraciÃ³n no tiene nada que ver\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QWK8kC0ujfM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55a82d60-c681-4514-8ee6-d26997a35465"
      },
      "source": [
        "prompt = \"I'm really hungry.\"\n",
        "next_sentence = \"I'm getting a BigMac.\"\n",
        "evaluar_oraciones(prompt,next_sentence)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La oraciÃ³n es una continuaciÃ³n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IziEtXRvs0i",
        "colab_type": "text"
      },
      "source": [
        "## Esto funciona bastante bien, que pasa si quiero entrenarlo para una tarea en especifico?\n",
        "\n",
        "Para esto debemos fine-tunear el modelo con nuestros datos. TomÃ© [este](https://medium.com/swlh/painless-fine-tuning-of-bert-in-pytorch-b91c14912caa) tutorial como referencia por si algÃºn paso no queda lo suficientemente claro. Vamos a fine-tunear BERT para realizar sentiment classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecFw688fxbVE",
        "colab_type": "text"
      },
      "source": [
        "Lo primero es inicializar un modelo de BERT sin fine-tunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go2YCpqZxmBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2mMoB7qxiTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creamos un modelo de BERT limpio\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "#El mismo tokenizador de antes\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IONb3bWNxBi4",
        "colab_type": "text"
      },
      "source": [
        "Debemos entender como se le pasan los datos a BERT. Imaginemos que queremos agregar varias oraciones simultaneamente. Como lo hacemos con tensores si tienen largo distinto? La solucion a esto se llama padding, es decir agregar tokens para que todas las secuencias tengan el mismo largo. \\\n",
        "Pero esto podrÃ­a traer problemas si es que BERT llegase a interpretar estos tokens como partes de la oraciÃ³n verdad? Para eso es que es necesario especificarle a BERT cuales son los tokens a los que les tiene que tomar atenciÃ³n.\n",
        "\n",
        "Por ejemplo, si tuviesemos un largo mÃ¡ximo de 12 tokens, para padear la oraciÃ³n 'I really enjoyed this movie a lot.' hariamos lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sVnR_TOw9e5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d0ef6e7f-d800-474d-a931-78a9a0d8fea1"
      },
      "source": [
        "#Largo maximo de los tokens\n",
        "T = 12\n",
        "sentence = 'I really enjoyed this movie a lot.'\n",
        "#Step 1: Tokenizar\n",
        "tokens = tokenizer.tokenize(sentence) # ['i', 'really', 'enjoyed', 'this', 'movie', 'a', 'lot', '.']\n",
        "#Step 2: Agregar [CLS] y [SEP]\n",
        "tokens = ['[CLS]'] + tokens + ['[SEP]'] # ['[CLS]','i', 'really', 'enjoyed', 'this', 'movie', 'a', 'lot', '.', '[SEP]']\n",
        "#Step 3: Padear tokens\n",
        "padded_tokens = tokens + ['[PAD]' for _ in range(T - len(tokens))] #    ['[CLS]','i', 'really', 'enjoyed', 'this', 'movie', 'a', 'lot', '.', '[SEP]', '[PAD]', ... , '[PAD]']\n",
        "attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens] # [    1  , 1 ,    1    ,    1     ,   1   ,    1   ,  1 ,   1  ,  1 ,   1    ,    0   , ... ,    0   ] \n",
        "#Step 4: Segment ids: Estos representan cuando tienes 2 oraciones, la primera se llena con 0's y la segunda con 1's\n",
        "seg_ids = [0 for _ in range(len(padded_tokens))] # En este caso no la usaremos, ya que es solo 1 oraciÃ³n. Su representacion son solo 0's\n",
        "#Step 5: Cambiamos los tokens por su respectivo numero, CLS = 101, SEP = 102, etc...\n",
        "token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
        "\n",
        "#Los cambiamos a tensores de pytorch antes de que entren al modelo, y es necesario agregarles una dimension extra.\n",
        "# Esta dimension representa cuantas oraciones estamos pasando\n",
        "token_ids = torch.tensor(token_ids).unsqueeze(0) #Shape : [1, 12]\n",
        "attn_mask = torch.tensor(attn_mask).unsqueeze(0) #Shape : [1, 12]\n",
        "seg_ids   = torch.tensor(seg_ids).unsqueeze(0) #Shape : [1, 12]\n",
        "\n",
        "#Y al igual que antes podemos pasarselos a BERT\n",
        "hidden_reps, cls_head = bert_model(token_ids, attention_mask = attn_mask, token_type_ids = seg_ids)\n",
        "print(hidden_reps.shape)\n",
        "#Out: torch.Size([1, 12, 768])\n",
        "print(cls_head.shape)\n",
        "#Out: torch.Size([1, 768])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1853-AW09Up",
        "colab_type": "text"
      },
      "source": [
        "No estÃ¡ de mÃ¡s agregar que BERT tiene un maximo de 512 tokens por input, por lo que si queremos agregar un texto muy grande debemos o truncarlo o separarlo en 2.\\\n",
        "Ahora que aprendimos como paddear oraciones, utilizaremos el Stanford Sentiment Tree Bank dataset que contiene movie reviews con sentimiento positivo (1) y negativo (0).\\\n",
        "Primero crearemos una clase para cargar los datos, extendiendo la clase Dataset que viene con pytorch:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1q9tjbz1tm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "class SSTDataset(Dataset):\n",
        "    # Inicializacion de la clase\n",
        "    def __init__(self, filename, maxlen):\n",
        "        #Guardar los contenidos del dataframe\n",
        "        self.df = pd.read_csv(filename, delimiter = '\\t')\n",
        "        #Initialize the BERT tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        # Establecer el largo mÃ¡ximo\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    # Funcion auxiliar que retorna el largo del dataframe\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #Seleccionamos la oracion y el label de este dataset en especifico.\n",
        "        sentence = self.df.loc[index, 'sentence']\n",
        "        label = self.df.loc[index, 'label']\n",
        "\n",
        "        #Realizamos todo el pre-procesamiento que explicamos anteriormente\n",
        "        tokens = self.tokenizer.tokenize(sentence)\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "        if len(tokens) < self.maxlen: # Comparamos con la cantidad maxima de tokens que dimos\n",
        "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] # Si es mas corta agregamos padding\n",
        "        else:\n",
        "            tokens = tokens[:self.maxlen-1] + ['[SEP]']  # Si es mas larga la cortamos\n",
        "\n",
        "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Utilizamos el tokenizador para pasarlos a id\n",
        "        tokens_ids_tensor = torch.tensor(tokens_ids) #Pasamos a tensor de pytorch\n",
        "\n",
        "        #1 para los tokens no padeados, 0 si es padding\n",
        "        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "        return tokens_ids_tensor, attn_mask, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVwxAu893Swd",
        "colab_type": "text"
      },
      "source": [
        "Creamos los dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0WWC2Be3UtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#Creamos instancias del training y validation sets\n",
        "train_set = SSTDataset(filename = 'train.tsv', maxlen = 30)\n",
        "val_set = SSTDataset(filename = 'dev.tsv', maxlen = 30)\n",
        "\n",
        "#Creamos los dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size = 64, num_workers = 5)\n",
        "val_loader = DataLoader(val_set, batch_size = 64, num_workers = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIHYBYjP38X3",
        "colab_type": "text"
      },
      "source": [
        "Ahora la parte mÃ¡s importante, generar nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beYZGXu_4BAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, freeze_bert = True):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        #Creamos una instancia de BERT sin entrenamiento previo\n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased').cuda()\n",
        "        \n",
        "        #Con esto podemos bloquear el entrenamiento de BERT, para comparar incluyendo el entrenamiento de bert y sin\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "        \n",
        "        #La capa para clasificar\n",
        "        #La idea es transformar una representacion de BERT (768 dimensiones) en 1 o 0 que representa el sentimiento\n",
        "        self.cls_layer = nn.Linear(768, 1).cuda()\n",
        "\n",
        "    def forward(self, seq, attn_masks):\n",
        "        '''\n",
        "        Inputs:\n",
        "            seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "        #Le pasamos el input al modelo BERT\n",
        "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "\n",
        "        #Obtenemos la representacion del token CLS\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        #Pasamos el token CLS por la capa de clasificacion\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkRDKgHj496y",
        "colab_type": "text"
      },
      "source": [
        "Utilizaremos binary cross-entropy loss y un descenso de gradiente estocastico"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7lXDEbk5OAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "#Creamos el classificador de sentimiento basado en BERT\n",
        "net_freezed = SentimentClassifier(freeze_bert = True)\n",
        "net_not_freezed = SentimentClassifier(freeze_bert = False)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti_freezed = optim.Adam(net_freezed.parameters(), lr = 2e-5)\n",
        "opti_not_freezed = optim.Adam(net_not_freezed.parameters(), lr = 2e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf2qL53wBN3_",
        "colab_type": "text"
      },
      "source": [
        "Agregamos funciones axuliares para medir el desempeÃ±o del entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Vk-syTBSUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "    \n",
        "def evaluate(net, criterion, dataloader):\n",
        "    net.eval() # Modo evaluacion del modelo, pesos no serÃ¡n modificados\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "    with torch.no_grad(): # Los gradientes no serÃ¡n guardados tampoco\n",
        "        for seq, attn_masks, labels in dataloader:\n",
        "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "            logits = net(seq, attn_masks)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT0-p2BE5c-T",
        "colab_type": "text"
      },
      "source": [
        "Y con la siguiente funcion entrenamos los parametros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32HzrhSG5euh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, criterion, opti, train_loader, val_loader, epochs):\n",
        "    for ep in range(epochs): # Iterador de las epocas\n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Enviamos los tensores a la GPU\n",
        "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "\n",
        "            #Evaluamos nuestro modelo en la secuencia y la mask de atencion\n",
        "            logits = net(seq, attn_masks)\n",
        "\n",
        "            #Calculamos la loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            #Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            #Optimization step\n",
        "            # Ojo que si no tenemos freeze_bert en true, vamos a entrenar los parametros de bert tambien.\n",
        "            opti.step()\n",
        "\n",
        "            if (it + 1) % 100 == 0:\n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss : {} Train Accuracy : {}\".format(it+1, ep+1, loss.item(), acc))\n",
        "        val_acc, val_loss = evaluate(net, criterion, val_loader)\n",
        "        print(\"Epoch {} complete! Validation Accuracy : {}, Validation Loss : {}\".format(ep+1, val_acc, val_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsXNXjQY6aKt",
        "colab_type": "text"
      },
      "source": [
        "Y finalmente entrenamos ambos modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcMLMjiz6ccC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "67852a69-c293-42ed-b785-d41d5760b193"
      },
      "source": [
        "epochs = 5\n",
        "train(net_freezed, criterion, opti_freezed, train_loader, val_loader, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 100 of epoch 1 complete. Loss : 0.6795556545257568 Train Accuracy : 0.515625\n",
            "Iteration 200 of epoch 1 complete. Loss : 0.6824479103088379 Train Accuracy : 0.609375\n",
            "Iteration 300 of epoch 1 complete. Loss : 0.6705337762832642 Train Accuracy : 0.609375\n",
            "Iteration 400 of epoch 1 complete. Loss : 0.6894826889038086 Train Accuracy : 0.484375\n",
            "Iteration 500 of epoch 1 complete. Loss : 0.6393808126449585 Train Accuracy : 0.671875\n",
            "Iteration 600 of epoch 1 complete. Loss : 0.6691819429397583 Train Accuracy : 0.5625\n",
            "Iteration 700 of epoch 1 complete. Loss : 0.6331959962844849 Train Accuracy : 0.640625\n",
            "Iteration 800 of epoch 1 complete. Loss : 0.6230830550193787 Train Accuracy : 0.6875\n",
            "Iteration 900 of epoch 1 complete. Loss : 0.6763279438018799 Train Accuracy : 0.578125\n",
            "Iteration 1000 of epoch 1 complete. Loss : 0.6254374980926514 Train Accuracy : 0.71875\n",
            "Epoch 1 complete! Validation Accuracy : 0.7183035612106323, Validation Loss : 0.6326529255935124\n",
            "Iteration 100 of epoch 2 complete. Loss : 0.6124631762504578 Train Accuracy : 0.6875\n",
            "Iteration 200 of epoch 2 complete. Loss : 0.6402492523193359 Train Accuracy : 0.703125\n",
            "Iteration 300 of epoch 2 complete. Loss : 0.6095575094223022 Train Accuracy : 0.75\n",
            "Iteration 400 of epoch 2 complete. Loss : 0.6320027112960815 Train Accuracy : 0.671875\n",
            "Iteration 500 of epoch 2 complete. Loss : 0.585234522819519 Train Accuracy : 0.78125\n",
            "Iteration 600 of epoch 2 complete. Loss : 0.6198886036872864 Train Accuracy : 0.65625\n",
            "Iteration 700 of epoch 2 complete. Loss : 0.5745449662208557 Train Accuracy : 0.734375\n",
            "Iteration 800 of epoch 2 complete. Loss : 0.5717377066612244 Train Accuracy : 0.8125\n",
            "Iteration 900 of epoch 2 complete. Loss : 0.6225888729095459 Train Accuracy : 0.703125\n",
            "Iteration 1000 of epoch 2 complete. Loss : 0.5685945153236389 Train Accuracy : 0.8125\n",
            "Epoch 2 complete! Validation Accuracy : 0.8024553656578064, Validation Loss : 0.5782724065440041\n",
            "Iteration 100 of epoch 3 complete. Loss : 0.563277542591095 Train Accuracy : 0.75\n",
            "Iteration 200 of epoch 3 complete. Loss : 0.6136597394943237 Train Accuracy : 0.65625\n",
            "Iteration 300 of epoch 3 complete. Loss : 0.5636260509490967 Train Accuracy : 0.78125\n",
            "Iteration 400 of epoch 3 complete. Loss : 0.5873987078666687 Train Accuracy : 0.75\n",
            "Iteration 500 of epoch 3 complete. Loss : 0.5442982912063599 Train Accuracy : 0.859375\n",
            "Iteration 600 of epoch 3 complete. Loss : 0.5815848112106323 Train Accuracy : 0.703125\n",
            "Iteration 700 of epoch 3 complete. Loss : 0.5297887325286865 Train Accuracy : 0.84375\n",
            "Iteration 800 of epoch 3 complete. Loss : 0.5327529907226562 Train Accuracy : 0.828125\n",
            "Iteration 900 of epoch 3 complete. Loss : 0.5802750587463379 Train Accuracy : 0.765625\n",
            "Iteration 1000 of epoch 3 complete. Loss : 0.5259298086166382 Train Accuracy : 0.828125\n",
            "Epoch 3 complete! Validation Accuracy : 0.8136160969734192, Validation Loss : 0.536571877343314\n",
            "Iteration 100 of epoch 4 complete. Loss : 0.5247408151626587 Train Accuracy : 0.78125\n",
            "Iteration 200 of epoch 4 complete. Loss : 0.5958994030952454 Train Accuracy : 0.625\n",
            "Iteration 300 of epoch 4 complete. Loss : 0.5284031629562378 Train Accuracy : 0.78125\n",
            "Iteration 400 of epoch 4 complete. Loss : 0.5541934967041016 Train Accuracy : 0.78125\n",
            "Iteration 500 of epoch 4 complete. Loss : 0.5123809576034546 Train Accuracy : 0.828125\n",
            "Iteration 600 of epoch 4 complete. Loss : 0.5521780848503113 Train Accuracy : 0.75\n",
            "Iteration 700 of epoch 4 complete. Loss : 0.4954630732536316 Train Accuracy : 0.859375\n",
            "Iteration 800 of epoch 4 complete. Loss : 0.5028725266456604 Train Accuracy : 0.828125\n",
            "Iteration 900 of epoch 4 complete. Loss : 0.5467398166656494 Train Accuracy : 0.765625\n",
            "Iteration 1000 of epoch 4 complete. Loss : 0.49396711587905884 Train Accuracy : 0.828125\n",
            "Epoch 4 complete! Validation Accuracy : 0.8111607432365417, Validation Loss : 0.5043546855449677\n",
            "Iteration 100 of epoch 5 complete. Loss : 0.494260311126709 Train Accuracy : 0.78125\n",
            "Iteration 200 of epoch 5 complete. Loss : 0.5842652320861816 Train Accuracy : 0.671875\n",
            "Iteration 300 of epoch 5 complete. Loss : 0.5010598301887512 Train Accuracy : 0.78125\n",
            "Iteration 400 of epoch 5 complete. Loss : 0.5293401479721069 Train Accuracy : 0.78125\n",
            "Iteration 500 of epoch 5 complete. Loss : 0.487056165933609 Train Accuracy : 0.859375\n",
            "Iteration 600 of epoch 5 complete. Loss : 0.5296634435653687 Train Accuracy : 0.78125\n",
            "Iteration 700 of epoch 5 complete. Loss : 0.4689977467060089 Train Accuracy : 0.875\n",
            "Iteration 800 of epoch 5 complete. Loss : 0.47979965806007385 Train Accuracy : 0.828125\n",
            "Iteration 900 of epoch 5 complete. Loss : 0.5198256373405457 Train Accuracy : 0.765625\n",
            "Iteration 1000 of epoch 5 complete. Loss : 0.46990013122558594 Train Accuracy : 0.796875\n",
            "Epoch 5 complete! Validation Accuracy : 0.8189732432365417, Validation Loss : 0.47924437054565977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxBUZUC6d1jC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "4d07370e-c782-4b0a-8d5d-23ef46f0794b"
      },
      "source": [
        "train(net_not_freezed, criterion, opti_not_freezed, train_loader, val_loader, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 100 of epoch 1 complete. Loss : 0.2919929623603821 Train Accuracy : 0.890625\n",
            "Iteration 200 of epoch 1 complete. Loss : 0.4610022008419037 Train Accuracy : 0.796875\n",
            "Iteration 300 of epoch 1 complete. Loss : 0.3382772207260132 Train Accuracy : 0.875\n",
            "Iteration 400 of epoch 1 complete. Loss : 0.14136575162410736 Train Accuracy : 0.921875\n",
            "Iteration 500 of epoch 1 complete. Loss : 0.0685427188873291 Train Accuracy : 0.984375\n",
            "Iteration 600 of epoch 1 complete. Loss : 0.15615978837013245 Train Accuracy : 0.921875\n",
            "Iteration 700 of epoch 1 complete. Loss : 0.2990710735321045 Train Accuracy : 0.875\n",
            "Iteration 800 of epoch 1 complete. Loss : 0.06944824755191803 Train Accuracy : 0.984375\n",
            "Iteration 900 of epoch 1 complete. Loss : 0.13161134719848633 Train Accuracy : 0.96875\n",
            "Iteration 1000 of epoch 1 complete. Loss : 0.12836754322052002 Train Accuracy : 0.96875\n",
            "Epoch 1 complete! Validation Accuracy : 0.9058035612106323, Validation Loss : 0.25508606327431543\n",
            "Iteration 100 of epoch 2 complete. Loss : 0.07849311828613281 Train Accuracy : 0.953125\n",
            "Iteration 200 of epoch 2 complete. Loss : 0.16527271270751953 Train Accuracy : 0.96875\n",
            "Iteration 300 of epoch 2 complete. Loss : 0.2586386203765869 Train Accuracy : 0.921875\n",
            "Iteration 400 of epoch 2 complete. Loss : 0.044397495687007904 Train Accuracy : 0.984375\n",
            "Iteration 500 of epoch 2 complete. Loss : 0.06985306739807129 Train Accuracy : 0.984375\n",
            "Iteration 600 of epoch 2 complete. Loss : 0.054696738719940186 Train Accuracy : 0.984375\n",
            "Iteration 700 of epoch 2 complete. Loss : 0.16971246898174286 Train Accuracy : 0.953125\n",
            "Iteration 800 of epoch 2 complete. Loss : 0.005608794279396534 Train Accuracy : 1.0\n",
            "Iteration 900 of epoch 2 complete. Loss : 0.02981206402182579 Train Accuracy : 0.984375\n",
            "Iteration 1000 of epoch 2 complete. Loss : 0.08376139402389526 Train Accuracy : 0.96875\n",
            "Epoch 2 complete! Validation Accuracy : 0.9035714268684387, Validation Loss : 0.2673555479518005\n",
            "Iteration 100 of epoch 3 complete. Loss : 0.024941155686974525 Train Accuracy : 1.0\n",
            "Iteration 200 of epoch 3 complete. Loss : 0.07665261626243591 Train Accuracy : 0.953125\n",
            "Iteration 300 of epoch 3 complete. Loss : 0.24904902279376984 Train Accuracy : 0.9375\n",
            "Iteration 400 of epoch 3 complete. Loss : 0.055118151009082794 Train Accuracy : 0.984375\n",
            "Iteration 500 of epoch 3 complete. Loss : 0.004846522118896246 Train Accuracy : 1.0\n",
            "Iteration 600 of epoch 3 complete. Loss : 0.017122505232691765 Train Accuracy : 1.0\n",
            "Iteration 700 of epoch 3 complete. Loss : 0.10792330652475357 Train Accuracy : 0.953125\n",
            "Iteration 800 of epoch 3 complete. Loss : 0.0024116889107972383 Train Accuracy : 1.0\n",
            "Iteration 900 of epoch 3 complete. Loss : 0.004038392566144466 Train Accuracy : 1.0\n",
            "Iteration 1000 of epoch 3 complete. Loss : 0.09052665531635284 Train Accuracy : 0.953125\n",
            "Epoch 3 complete! Validation Accuracy : 0.8991071581840515, Validation Loss : 0.294412513928754\n",
            "Iteration 100 of epoch 4 complete. Loss : 0.0038735780399292707 Train Accuracy : 1.0\n",
            "Iteration 200 of epoch 4 complete. Loss : 0.020893648266792297 Train Accuracy : 0.984375\n",
            "Iteration 300 of epoch 4 complete. Loss : 0.12112196534872055 Train Accuracy : 0.96875\n",
            "Iteration 400 of epoch 4 complete. Loss : 0.028255373239517212 Train Accuracy : 0.984375\n",
            "Iteration 500 of epoch 4 complete. Loss : 0.003907563164830208 Train Accuracy : 1.0\n",
            "Iteration 600 of epoch 4 complete. Loss : 0.01481613703072071 Train Accuracy : 1.0\n",
            "Iteration 700 of epoch 4 complete. Loss : 0.06262359768152237 Train Accuracy : 0.953125\n",
            "Iteration 800 of epoch 4 complete. Loss : 0.0021003049332648516 Train Accuracy : 1.0\n",
            "Iteration 900 of epoch 4 complete. Loss : 0.00777034368366003 Train Accuracy : 1.0\n",
            "Iteration 1000 of epoch 4 complete. Loss : 0.01746947318315506 Train Accuracy : 1.0\n",
            "Epoch 4 complete! Validation Accuracy : 0.8881696462631226, Validation Loss : 0.4397291157926832\n",
            "Iteration 100 of epoch 5 complete. Loss : 0.00885507371276617 Train Accuracy : 1.0\n",
            "Iteration 200 of epoch 5 complete. Loss : 0.053689926862716675 Train Accuracy : 0.96875\n",
            "Iteration 300 of epoch 5 complete. Loss : 0.1083274558186531 Train Accuracy : 0.96875\n",
            "Iteration 400 of epoch 5 complete. Loss : 0.025958016514778137 Train Accuracy : 0.984375\n",
            "Iteration 500 of epoch 5 complete. Loss : 0.001841120421886444 Train Accuracy : 1.0\n",
            "Iteration 600 of epoch 5 complete. Loss : 0.004323156550526619 Train Accuracy : 1.0\n",
            "Iteration 700 of epoch 5 complete. Loss : 0.02931322529911995 Train Accuracy : 0.984375\n",
            "Iteration 800 of epoch 5 complete. Loss : 0.005530685186386108 Train Accuracy : 1.0\n",
            "Iteration 900 of epoch 5 complete. Loss : 0.001964530674740672 Train Accuracy : 1.0\n",
            "Iteration 1000 of epoch 5 complete. Loss : 0.0035197387915104628 Train Accuracy : 1.0\n",
            "Epoch 5 complete! Validation Accuracy : 0.8857142925262451, Validation Loss : 0.42530072799750734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giFhU398k17w",
        "colab_type": "text"
      },
      "source": [
        "Comparando ambos entrenamientos, pasamos de 82% a 88% en 5 epocas. El primer entrenamiento solo entrenamos la capa de clasificacion mientras que en el segundo tambien modificamos los parametros de BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exBniyWo8Ni9",
        "colab_type": "text"
      },
      "source": [
        "Este ultimo entrenamiento es lo que llamamos fine-tunning. Lo mÃ¡s importante de esto es que no necesitamos un super computador para poder mejorar las representaciones en una tarea especifica."
      ]
    }
  ]
}